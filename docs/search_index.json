[["index.html", "16S Microbiome Analysis on a Computing Cluster Welcome!", " 16S Microbiome Analysis on a Computing Cluster Johanna Bosch 2023-10-23 Welcome! This book can be used to learn how to conduct a comparative 16S amplicon analysis using QIIME2 on the Digital Research Alliance of Canada’s Graham cluster. All of the references in this book are cited by number, you can find the References section at the end of this book. The project methods described here are formally reviewed and cited in Bosch J. 2023 From Seabirds to Sediments: The ecological footprint of seabirds at a prominent North Atlantic breeding colony tracked using a multi-proxy paleolimnological approach [unpubl.]. Newfoundland, CA: Memorial University of Newfoundland [1]. Here, I cover each command and tool that I used in the analysis, step-by-step. I wanted to make an analysis review that helps new users, like me, get through the nitty-gritty process of learning how to use a computing cluster to analyze genomic data. The review is in a tutorial format, with explanations on how I did things but also ways you could improve the analysis or even make it your own. I analyzed my amplicon sequencing data using QIIME2 on Compute Canada’s Graham cluster, a resource offered by the Digital Alliance of Canada. This method of analysis offers several advantages, including efficient processing of large datasets, access to a collaborative community of researchers, an extremely helpful customer support team, and, most importantly, the flexibility of accessing files and performing analyses from any laptop or desktop computer with internet access.   Github: johannabosch yohannabosch@gmail.com   "],["introduction.html", "1 INTRODUCTION 1.1 A brief project review 1.2 DNA analysis", " 1 INTRODUCTION All of the methods and protocols discussed in this analysis review are outlined in Bosch et al. 2023 [1] 1.1 A brief project review The data used in this analysis is from a project that focuses on the transfer of seabird nutrients from a seabird nesting colony to nearby freshwater ponds [1]. View the figure below to understand the ecological processes taking place in my study sites. Figure 1.1: Seabird nutrient transfer from Bird Rock, in Cape St. Mary’s Ecological Reserve (Newfoundland) to a pond 240 m away from the seabird colony. Seen in the illustration are northern gannets (Morus bassanus) and black-legged kittiwakes (Rissa tridactyla), which both nest within the Reserve on an annual basis Using targeted 16S amplicon sequencing, we first assessed the gut microbiome composition of 4 species of seabirds that nest in Newfoundland. Next, we assessed how the bacterial composition of sediments, taken from ponds nearby a colony, are affected by seabird nutrient transfer. 1.1.1 Study species and sites Two different types of samples are used in the analysis, including seabird fecal swabs that were collected by an experienced seabird handler according to proper animal care handling techniques, and sediment subsamples that were taken from sediment cores collected from freshwater ponds. Seabird fecal swabs = 19 total Collected from 4 species of seabird nesting in Newfoundland: * northern gannets (Morus bassanus) * black-legged kittiwakes (Rissa tridactyla) * common murres (Uria aalge) * atlantic puffins (Fratercula arctica) Sediment core subsamples = 12 total Collected from subsamples of sediment cores taken from 3 ponds in Newfoundland: * CSM impacted pond ~240m upwind of a nesting colony (Bird Rock, Cape St. Mary’s (CSM), Newfoundland) made up of gannets, kittiwakes and murres * CSM reference pond ~2.56 km away from the multispecies colony * Little Fogo Island pond, another impacted pond, directly adjacent to an atlantic puffin colony 1.1.2 Sequencing &amp; library prep The samples were isolated using a DNA isolation kit and PCR amplified using the V4V5 primers (515FB = GTGYCAGCMGCCGCGGTAA,926R = CCGYCAATTYMTTTRAGTTT)[2] [3]. Samples were sequenced by the Integrated Microbiome Resource (IMR) - you can learn more about the sequencing and library details from IMR on their (protocols page)[https://imr.bio/protocols.html]. A more in depth explanation is provided in the published project [1]. 1.1.3 Files after sequencing You can download the files used in this analysis here: rawdata-birds.zip file - rawdata-sediments.zip file - I keep these files in a folder titled QIIME_files on my Desktop. I began with demultiplexed raw sequence data in paired-end .FASTQ format. There are two files for each sample (forwards reads and reverse reads), and 31 samples total, so we have 62 files that are each compressed into GZ format. Each file’s name follows a standard naming convention for .FASTQ files, for example the samples for a northern gannet (NOGA), common murre (COMU), and atlantic puffin (ATPU) sample were named as follows: Forward reads | Reverse reads NOGA27_S100_L001_R1_001.fastq.gz | NOGA27_S100_L001_R2_001.fastq.gz COMU33_S110_L001_R1_001.fastq.gz | COMU33_S100_L001_R2_001.fastq.gz ATPU05_S111_L001_R1_001.fastq.gz | ATPU05_S111_L001_R2_001.fastq.gz Here, the sample NOGA27 is broken down into 5 parts. The sample ID number NOGA27, the sample number S100_, the lane number L001_, the direction of the reads (forward: R1, reverse: R2), the last segment which will always end with 001, the file format .FASTA or .FASTQ depending on the file format (.FASTA contains only reads, .FASTQ contains quality information). The .gz at the end of each file name means the file is compressed with a standard GNU zip (gzip) compression algorithm - there are tools and techniques for working with the compressed files without ever un-compressing them, but in the first section of this analysis we unzip the data files and work with decompressed data. If you don’t decompress data before running an analysis, tools that work with compressed data almost always decompress the data (in memory) anyways before running a task. ___ 1.2 DNA analysis For the DNA analysis, we use a variety of tools and software available on the Graham cluster, including QIIME2, a next-generation microbiome bioinformatics suite that offers a free, user-friendly, and open source platform for both advanced and beginner researchers [4]. The analysis using QIIME2 is easily run on the Graham cluster hosted by the Alliance. Platforms like QIIME2 are made available through the cluster by loading environment management systems (modules). These modules allow users to access a consistent and controlled environment for executing commands using different programming languages (R, Python, MATLAB, C/C++, Java, FORTRAN, and Julia). For a list of resources used in this tutorial, refer to Ch-10 (Resources). 1.2.1 The Graham cluster The first portion of this tutorial reviews using the Graham cluster hosted by the Digital Research Alliance of Canada. Graham is a heterogeneous cluster, suitable for a variety of workloads, and located at the University of Waterloo. The cluster is useful for running jobs that are intensive; jobs are run as a simple text file that contains information about which allocation to run the job on, and let’s you specify how many compute nodes the job needs, how much memory the job needs, and how long the job will take to run. Using a computing cluster is also advantageous because you can log-in to their nodes from any computer, store data and make use of their many client support services. Here are some of the useful Wiki pages hosted by the Alliance that you should read if you are a new user: Getting started - https://docs.alliancecan.ca/wiki/Getting_started the Graham cluster - https://docs.alliancecan.ca/wiki/Graham System status - https://status.alliancecan.ca/ Running jobs - https://docs.alliancecan.ca/wiki/Running_jobs 1.2.2 Data accessibility If you are following along as a tutorial, you can download all of the files I used for this analysis through the (Github repo)[https://github.com/johannabosch/QIIME2_for_Graham/tree/main], or you can use your own files and adjust the commands as you go. NOTE: Download and take a look at my metadata files on the Github repo, they contain important information about each sample. If you’re following along with this analysis, make a folder on your local computer called QIIME_files to keep all of the metadata and rawdata files in. This is also where all of your files from the QIIME output will live. Visit this Q2book to learn more about how to format your metadata files and use them in your analysis. If you don’t have your own raw data to work with, you can download the rawdata file titled rawdata.zip for this analysis from this Google drive: https://drive.google.com/drive/folders/1mwfqLjyuLIpHO0KuhRAuxgYXjnbp1iQH?usp=sharing   Github: johannabosch yohannabosch@gmail.com   "],["compute-canada-basics.html", "2 COMPUTE CANADA BASICS 2.1 Cluster computing basics 2.2 Job scripts ", " 2 COMPUTE CANADA BASICS This research was enabled by support provided by the Digital Research Alliance of Canada. The following section reviews the basics of accessing and using the Graham cluster. To conduct this analysis a user account for the Graham cluster has to be created and verified by the PI of the account. You can read more about applying for an account with the Digital Research Alliance here: [https://www.computecanada.ca/research-portal/account-management/apply-for-an-account/] 2.1 Cluster computing basics The following is a brief review of the ins-and-outs of using the cluster on your command terminal and the commands used often in this analysis. 2.1.1 Signing into the cluster To begin using the cluster, open your command prompt or terminal and sign into the Graham cluster #use an `ssh` command to log-in and press enter ssh &#39;username&#39;@graham.computecanada.ca # type in the invisible password and press Enter NOTE: Replace 'username' in the above command, for example janesmith@graham.computecananda.ca 2.1.2 Basic computing commands Your user directory can be found within your PI’s project file, which typically begins with the def- prefix, like this: #use the cd command to move to your user directory cd projects/&#39;PI-profile&#39;/&#39;username&#39; #to move up to the parent directory, use .. cd .. #to copy a directory named &#39;rawdata&#39; use cp cp projects/&#39;PI-profile&#39;/&#39;username&#39;/rawdata NOTE:You can press the TAB button while writing a file path to complete a path name, or double tap to show a list of options in the working directory DANGER ZONE Use these commands to delete files cautiously: #to remove a file called &#39;metadata.txt&#39; from a directory use rm rm projects/&quot;&lt;PI-project-profile-name&quot;&gt;/&lt;&quot;username&quot;&gt;/rawdata/metadata/metadata.txt #to remove a directory called &#39;metadata&#39; use rm -r rm -r projects/&quot;&lt;PI-project-profile-name&quot;&gt;/&lt;&quot;username&quot;&gt;/metadata 2.1.3 Loading modules Many softwares are already installed on the cluster and can be used by loading modules - sometimes multiple modules have to be loaded first. To view a list of available softwares visit the Alliance (Available Software - Wiki page)[https://docs.alliancecan.ca/wiki/Available_software] You should always specify the version of the software you want to load, and this will require you to pull a list of all the versions available on the cluster. To load a module, use the following command(s): module load &#39;module-name/0.0.0&#39; # To see what modules need to be loaded for a software or tool, use: module spider &#39;module-name/0.0.0&#39; # For example, to see what other modules need to be loaded to run python, use: module spider python/3.11.2 To see what versions of python are offered on Graham, exclude the version: module spider python View the output of running module spider python/3.11.2 below. The output gives you a description of the module, and a properties section which lists other modules you will need, in this case StdEnv/2020. It then lists the extensions that are provided by python (v3/.11.2) - keep in mind that sometimes older versions of a tool will offer different extensions. ------------------------------------------------------------------------------------- python: python/3.11.2 ------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. Properties: Tools for development / Outils de développement You will need to load all module(s) on any one of the lines below before the &quot;python/3.11.2&quot; module is available to load. StdEnv/2020 This module provides the following extensions: distlib/0.3.6 (E), editables/0.3 (E), filelock/3.9.0 (E), flit_core/3.8.0 (E), hatch_vcs/0.3.0 (E), hatchling/1.13.0 (E), packaging/23.0 (E), pathspec/0.11.0 (E), pip/23.0 (E), platformdirs/2.6.2 (E), pluggy/1.0.0 (E), pyparsing/3.0.9 (E), setuptools/63.4.3 (E), setuptools_scm/7.1.0 (E), six/1.16.0 (E), tomli/2.0.1 (E), typing_extensions/4.5.0 (E), virtualenv/20.16.3 (E), wheel/0.38.4 (E) Help: Description =========== Python is a programming language that lets you work more quickly and integrate your systems more effectively. More information ================ - Homepage: https://python.org/ Included extensions =================== distlib-0.3.6, editables-0.3, filelock-3.9.0, flit_core-3.8.0, hatch_vcs-0.3.0, hatchling-1.13.0, packaging-23.0, pathspec-0.11.0, pip-23.0, platformdirs-2.6.2, pluggy-1.0.0, pyparsing-3.0.9, setuptools-63.4.3, setuptools_scm-7.1.0, six-1.16.0, tomli-2.0.1, typing_extensions-4.5.0, virtualenv-20.16.3, wheel-0.38.4 2.2 Job scripts A job is a set of commands written in a bash script (a small text file containing commands which specify what tools to run, the input files, and the output files. Job scripts are used on computing clusters to run commands and use various tools. A bash script is really just a plain text file containing a series of commands and instructions written in the Bash programming language, which is the default shell for many Unix-like operating systems, including the Graham cluster. To indicate that a file should be interpreted as a Bash script, it typically begins with the shebang line #!/bin/bash, which specifies the path to the Bash interpreter. This line informs the system how to execute the script, From the Digital Research Alliance Wiki: &gt; According to Compute Canada’s user code, a job script must be submitted to the cluster’s scheduler. Any and all processes should not be run on compute nodes except via the scheduler (any tasks that consume more than 10 CPU-minutes and approx. 4GB of ram). 2.2.1 Using Nano Bash scripts can be written in nano, a text editor hosted on the cluster. For example, to write a script called copy-file.sh in nano that will copy the file /metadata-birds.csv to the main directory, use the following: # Open nano using the following command: nano copy-file.sh Here are some basics about using nano: a navigation menu will pop up at the bottom of your terminal you can paste code into nano using CTRL+V or by right clicking 2.2.2 Writing a bash script Each script begins with ‘##!/bin/bash’. You can specify how much time the job will take and how much memory will be needed. You can also specify which allocation to run the job on, the number of compute nodes the job needs, and more. Typically, bash scripts are named with a .sh suffix. When it comes to memory, unfortunately Slurm interprets K, M, G, etc., as binary prefixes, so a --mem=125G is actually equivalent to --mem=128000M. Even though we would usually just copy files directly in the working directory, here is an example of a simple job script that would copy the file metadata-birds.csv to the main working directory (/amplicon-analysis): #!/bin/bash #SBATCH --time=01:00:00 #SBATCH --mem=1GB cp /metadata/metadata-birds.csv metadata-birds-copy.csv Once the script is written, you can press ctrl + X, name the script if you did not do it earlier when starting nano, save and exit. Usually scripts are named ending with .sh suffix. 2.2.3 Running scripts on the scheduler To run the script on the scheduler use an sbatch command followed by your file name (the output will display a JOBID if it runs correctly): sbatch copy-file.sh Submitted batch job 7365672 Here are some other important commands to know: # to check the status of your job sq -u &#39;username&#39; # to cancel a job that is being run (specify the JOBID) scancel 7365672 NOTE: to find out how much memory a job takes up so you can adjust your memory parameters in the future, use the seff command with the JOBID: seff 7365672 Every time you submit a job, a slurm file is created with the output of your job in your working directory. To view the results of your job script view the new slurm file in your working directory. ls view &lt;&quot;slurm file name&quot;&gt; When finished writing and running your job script, save it to a separate working directory called /scripts mkdir scripts mv &lt;copy-file.sh /scripts TIP: delete your slurm files each time one is created, this way when you want to view your job’s output more easily all you will have to type in your working directory is view slurm*   Github: johannabosch yohannabosch@gmail.com   "],["importing-data.html", "3 IMPORTING DATA 3.1 Getting organized 3.2 Moving data to the cluster 3.3 Organizing .FASTQ files", " 3 IMPORTING DATA To begin importing your data onto the cluster, first make sure the files are downloaded on your computer. Go to Ch-10 (Resources) to download the raw data used in this analysis if you haven’t already, or follow along using your own data! You will also want to download the accompanying metadata files. 3.1 Getting organized Before importing data onto the cluster, it’s nice to make sure you have a dedicated directory for all your data files. If you stay organized during your analysis you can pretty much run all your job scripts in the same directory. Make a directory to work in for this analysis mkdir amplicon-analysis # navigate to the new directory cd amplicon-analysis # make 2 more folders called **/rawdata** and **/metadata** mkdir rawdata mkdir metadata #make a file for the bird fecal samples in /rawdata mkdir rawdata/birds #make a file for the sediment samples in /rawdata mkdir rawdata/sediments Check to make sure the new folders are there using thels command 3.2 Moving data to the cluster Transferring data onto the cluster from your local computer desktop can be done using the safe file transfer protocol (SFTP). To do this, first exit the cluster and re-enter the using the following safe file transfer protocol (SFTP) command: #first exit the cluster if you are still signed in exit #now sign in using the SFTP command sftp &#39;username&#39;@graham.computecanada.ca # Now, move to the amplicon-analysis directory using the &#39;cd&#39; command cd projects/&#39;PI-profile&#39;/&#39;username&#39;/amplicon-analysis Next, navigate to the folder on your local computer where the raw data file raw-data.zip is stored, usually in a .zip or .gz format. To run a command in our local computer, always start the command with the letter l, like this: #print contents of the rawdata folder on your local computer lcd &lt;local path to folder containg .FASTQ files&gt; To move files from your local computer to the cluster: put &lt;filename&gt; To move files from the cluster to your local computer: get &lt;filename&gt; For example. in order to transfer the files from a local computer into a user directory on the cluster, use the command put with a wildcard expression to move all the .FASTQ.GZ files in the local directory. To later transfer the QIIME artifacts created in this analysis from the remote computer to our local computer, the get command is used. The /amplicon-analysis directory should contain a /rawdata folder at this point to move the raw data files into. #if you want to transfer the zipped folders into the /rawdata directory on the cluster: put rawdata-birds.zip rawdata put rawdata-sediments.zip rawdata #check to make sure all the files are properly transferred to the cluster ls rawdata/rawdata-birds ls rawdata/rawdata-sediments NOTE: * Use an astericks () as a “wildcard” in any regular expression to include multiple files in one command, or avoid writing the full filename over and over again while running commands (see the put command below) You can check that you’re in the right directory using the pwd or lpwd command After downloading all of the metadata files from the Github repo, transfer them over to the cluster like this: #transfer the metadata files lcd &lt;path to metadata files on local computer&gt; cd &lt;path to amplicon-analysis directory&gt; put metadata* metadata/ ## OUTPUT: Uploading metadata-birds.csv to /project/6000879/jlb686/amplicon-data/amplicon-analysis/metadata/metadata-birds.csv metadata-birds.csv 100% 585 8.9KB/s 00:00 Uploading metadata-grouped.csv to /project/6000879/jlb686/amplicon-data/amplicon-analysis/metadata/metadata-grouped.csv metadata-grouped.csv 100% 254 5.0KB/s 00:00 Uploading metadata-sediments.csv to /project/6000879/jlb686/amplicon-data/amplicon-analysis/metadata/metadata-sediments.csv metadata-sediments.csv 100% 537 4.6KB/s 00:00 Uploading metadata.csv to /project/6000879/jlb686/amplicon-data/amplicon-analysis/metadata/metadata.csv metadata.csv 100% 992 21.2KB/s 00:00 # check to make sure files are there ls metadata ## OUTPUT: metadata-birds.csv metadata-grouped.csv metadata-sediments.csv metadata.csv To proceed to the next steps, exit SFTP, log back into the cluster using ssh, and navigate back to your working directory exit ssh jlb686@graham.computecanada.ca cd projects/&#39;PI-profile&#39;/&#39;username&#39;/amplicon-analysis 3.3 Organizing .FASTQ files 3.3.1 Unzip rawdata files If you need to unzip your compressed rawdata folder on the cluster, use the following command: unzip rawdata/&lt;filename&gt;.zip #replace &lt;filename&gt; with your file Unzip the rawdata files for the bird and sediment samples. If you want to see a portion of what is in one of the .fastq.gz rawdata files, you can use the following command: zcat rawdata/birds/ATPU01_S125_L001_R1_001.fastq.gz | head Check to make sure the files are in the correct folders using the ls command. Here I ran the command so that if you are working with my files you can see what the output should look like: ls rawdata/birds ## OUTPUT: ATPU01_S125_L001_R1_001.fastq.gz COMU-42_S185_L001_R1_001.fastq.gz ATPU01_S125_L001_R2_001.fastq.gz COMU-42_S185_L001_R2_001.fastq.gz ATPU02_S137_L001_R1_001.fastq.gz COMU43_S102_L001_R1_001.fastq.gz ATPU02_S137_L001_R2_001.fastq.gz COMU43_S102_L001_R2_001.fastq.gz ATPU03_S149_L001_R1_001.fastq.gz COMU44_S114_L001_R1_001.fastq.gz ATPU03_S149_L001_R2_001.fastq.gz COMU44_S114_L001_R2_001.fastq.gz ATPU56_S161_L001_R1_001.fastq.gz COMU46_S126_L001_R1_001.fastq.gz ATPU56_S161_L001_R2_001.fastq.gz COMU46_S126_L001_R2_001.fastq.gz ATPU58_S173_L001_R1_001.fastq.gz NOGA27_S100_L001_R1_001.fastq.gz ATPU58_S173_L001_R2_001.fastq.gz NOGA27_S100_L001_R2_001.fastq.gz BLKI01_S160_L001_R1_001.fastq.gz NOGA30_S112_L001_R1_001.fastq.gz BLKI01_S160_L001_R2_001.fastq.gz NOGA30_S112_L001_R2_001.fastq.gz BLKI02_S172_L001_R1_001.fastq.gz NOGA31_S124_L001_R1_001.fastq.gz BLKI02_S172_L001_R2_001.fastq.gz NOGA31_S124_L001_R2_001.fastq.gz BLKI03_S184_L001_R1_001.fastq.gz NOGA32_S136_L001_R1_001.fastq.gz BLKI03_S184_L001_R2_001.fastq.gz NOGA32_S136_L001_R2_001.fastq.gz BLKI04_S101_L001_R1_001.fastq.gz NOGA33_S148_L001_R1_001.fastq.gz BLKI04_S101_L001_R2_001.fastq.gz BLKI05_S113_L001_R1_001.fastq.gz BLKI05_S113_L001_R2_001.fastq.gz ___ ls rawdata/sediments ## OUTPUT: FOGO12_S140_L001_R1_001.fastq.gz REF12_S175_L001_R1_001.fastq.gz FOGO12_S140_L001_R2_001.fastq.gz REF12_S175_L001_R2_001.fastq.gz FOGO1_S187_L001_R1_001.fastq.gz REF1_S127_L001_R1_001.fastq.gz FOGO1_S187_L001_R2_001.fastq.gz REF1_S127_L001_R2_001.fastq.gz FOGO2_S104_L001_R1_001.fastq.gz REF2_S139_L001_R1_001.fastq.gz FOGO2_S104_L001_R2_001.fastq.gz REF2_S139_L001_R2_001.fastq.gz FOGO4_S116_L001_R1_001.fastq.gz REF4_S151_L001_R1_001.fastq.gz FOGO4_S116_L001_R2_001.fastq.gz REF4_S151_L001_R2_001.fastq.gz FOGO6_S128_L001_R1_001.fastq.gz REF8_S163_L001_R1_001.fastq.gz FOGO6_S128_L001_R2_001.fastq.gz REF8_S163_L001_R2_001.fastq.gz IMP12_S115_L001_R1_001.fastq.gz IMP12_S115_L001_R2_001.fastq.gz IMP1_S162_L001_R1_001.fastq.gz IMP1_S162_L001_R2_001.fastq.gz IMP2_S174_L001_R1_001.fastq.gz IMP2_S174_L001_R2_001.fastq.gz IMP4_S186_L001_R1_001.fastq.gz IMP4_S186_L001_R2_001.fastq.gz IMP-8_S103_L001_R1_001.fastq.gz IMP-8_S103_L001_R2_001.fastq.gz   Github: johannabosch yohannabosch@gmail.com   "],["qiime2-on-the-cluster.html", "4 QIIME2 ON THE CLUSTER 4.1 Use QIIME2 via Apptainer", " 4 QIIME2 ON THE CLUSTER Qiime2 is now an available module on the Graham cluster. Apptainer [5] is a container system that’s required to access QIIME2 because all qiime commands are executed within a containerized environment. Apptainer facilitates the management and execution of containers - a standard unit of software that packages up code and all its dependencies so an application like QIIME2 can run seamlessly. To employ the QIIME2 platform using Apptainer, users need to define specific folder bindings within the container by setting the APPTAINER_BIND environment variable. This ensures that the container can access and interact with the necessary data and directories. Additionally, Apptainer has replaced Singularity for this purpose, providing improved features and compatibility, all while preserving backward compatibility. 4.1 Use QIIME2 via Apptainer 4.1.1 Finding physical paths Apptainer allows you to map directories on your host system to directories within an Apptainer container using bind mounts. This enables seamless reading and writing of data between the host system and the container. On the cluster, it can be configured through user-defined bind paths that you specify in your code, providing flexibility in managing file access between the host and the container. To connect to and use Apptainer mounts, you have to define the path to your physical directory without using symbolic links (Apptainer is not compatible with symbolic links). What is a symbolic link? - When you navigate to your home directory on the cluster, usually you’ll use a command that looks something like this: cd projects/def-&lt;group-name&gt;/&lt;user-ID&gt;/. Let’s say your group name is birch, then the symbolic link to your project directory is def-birch. The symbolic links we use on the Graham, Béluga, Cedar, and Narval clusters serve as shortcuts or pointers to group-specific directories, making it easier for users to navigate and work within their respective project spaces. What is a physical path? - The physical path represents the real, tangible location of data on a storage medium.Symbolic paths are more about providing convenience and abstraction, while physical paths represent the concrete location of data on storage media. Given this info, let’s start by finding the physical path of our user directory with the ls -l or pwd -P command: #navigate to your user directory cd projects/def-&lt;group-name&gt;/&lt;user-ID&gt;/amplicon-analysis #use the `ls` command ls -l /home/jlb686/projects/ ## OUTPUT: total 0 lrwxrwxrwx 1 &lt;user-ID&gt; &lt;user-ID&gt; 16 Nov 10 2021 def-&lt;group-name&gt; -&gt; /project/6011879 # Alternatively, use `pwd -P`: pwd -P ## OUTPUT: /project/6011879/&lt;user-ID&gt;/amplicon-analysis The output of the ls -l or pwd -P command provides the physical path for def-&lt;group-name&gt;, which in this example is project/6011879. 4.1.2 Using Apptainer mounts Now that the physical path has been found, here is an example of a bash script using Apptainer to run a QIIME2 command. These scripts can easily be written using Nano. I discussed how to use Nano in Ch-02 (Computing Basics). The Apptainer mount is set to the /project/6011879/&lt;user-ID&gt;/amplicon-analysis directory. These QIIME2 scripts should always include the shebang line (#!/bin/bash), the amount of time memory to use, and the two export commands you see below: #!/bin/bash #SBATCH --time= #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 &#39;qiime command here&#39; The APPTAINER_BIND line connects a folder on your computer, /project/_____, with a folder inside the Apptainer container, /data/. It’s like creating a bridge between your files and the container, so you can easily access your data from within the container. Make sure to replace this example physical path (project/6011879) with your own physical path number. The APPTAINER_WORKDIR=${SLURM_TMPDIR} line tells the container where to do its work. Even though the container starts in a temporary directory (SLURM_TMPDIR), the earlier bind means you can still reach your original data from within the container through the /data/ folder. This way, you can keep your files organized and work on them effectively. So, you’re essentially using Apptainer to connect your data with the container and instructing the container to do its work in a temporary directory while still having easy access to your data. It’s like having a workspace inside a bigger office where your files are stored separately, but you can quickly grab what you need when you’re working. Once the script is written in nano, press ctrl + X, name the script (usually ending in .sh), save and exit. Here are some tips to run the script and perform other commands related to job scripts: # run the script sbatch &#39;script&#39;.sh # check the job status sq -u &#39;script&#39;.sh # check the slurm file once the job is complete view slurm-&#39;jobID&#39; #press the shift key while scrolling to view the whole file # check how much memory the build used for future reference seff &#39;jobID&#39; #move the script to the /scripts directory mv &#39;script&#39;.sh /scripts #remove your slurm file rm slurm-&#39;jobID&#39; 4.1.3 Getting help You can easily view all the options available for each QIIME2 command #load qiime module first module load qiime2 #use the --help option to view all the available options for a qiime command qiime --help __ You can read more about using QIIME2 on the Alliance (QIIME Wiki Page)[https://docs.alliancecan.ca/wiki/QIIME]. To learn more about Apptainer visit the Alliance (Apptainer Wiki page)[https://docs.alliancecan.ca/wiki/Apptainer].   Github: johannabosch yohannabosch@gmail.com   "],["getting-started-with-qiime2.html", "5 GETTING STARTED WITH QIIME2 5.1 Creating artifact files 5.2 Visualizing artifact files", " 5 GETTING STARTED WITH QIIME2 At this point in the analysis, QIIME2 should be ready to go and you should understand how to use Apptainer to run qiime commands. In this next section, I review how to take your rawdata files and convert them into a usable format for the QIIME2 platform. 5.1 Creating artifact files To manipulate and analyze sequencing data in QIIME2, all data has to be converted to a QIIME2 artifact file (.QZA) or visualization file (.QZV). This means all the sequencing data that is in .FASTQ files needs to be compiled into one .QZA formatted file. These artifact files are kept in the main directory for easy access, in this case /amplicon-analysis. 5.1.1 Import the .FASTQ files Start by moving to the /amplicon-analysis directory. Copy and paste the following script called import.sh into nano and edit accordingly: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-name&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for bird samples qiime tools import \\ --type &#39;SampleData[PairedEndSequencesWithQuality]&#39; \\ --input-path /data/amplicon-analysis/rawdata/birds/ \\ --input-format CasavaOneEightSingleLanePerSampleDirFmt \\ --output-path /data/amplicon-analysis/PEreads_birds.qza #for sediment samples qiime tools import \\ --type &#39;SampleData[PairedEndSequencesWithQuality]&#39; \\ --input-path /data/amplicon-analysis/rawdata/sediments/ \\ --input-format CasavaOneEightSingleLanePerSampleDirFmt \\ --output-path /data/amplicon-analysis/PEreads_sediments.qza Above, the command was written as follows: The data file types we used were denoted as SampleData[PairedEndSequencesWithQuality], they are paired end sequences with quality information in each file (hence the .FASTQ file format) The input type used was CasavaOneEightSingleLanePerSampleDirFmt There are two fastq.qz files for each sample in the study (forward or reverse reads for that sample). Run the bash script using the command sbatch import.sh in your working directory. The qiime import command can take awhile depending on how many samples you have and their size. Check to see that the script is still running using the sq -u &lt;user-ID&gt; command. sbatch import.sh #check the slurm file #check the artifact files are in the main directory 5.1.2 Inspect the .QZA files Now that all of our data for the bird and sediment samples are compiled into an artifact file, we can inspect it directly in the home directory using the qiime tools peek command: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-name&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #then running the peek command qiime tools peek /data/amplicon-analysis/PEreads_birds.qza qiime tools peek /data/amplicon-analysis/PEreads_sediments.qza The output within the slurm file that was produced after running the sbatch command will look something as follows: UUID: 23ced561-437b-44b9-bacb-1f24e7ebf19d #for the bird samples Type: SampleData[PairedEndSequencesWithQuality] Data format: SingleLanePerSamplePairedEndFastqDirFmt UUID: cdfbe42a-b687-4a5b-a1d2-e6d0205414a1 #for the sediment samples Type: SampleData[PairedEndSequencesWithQuality] Data format: SingleLanePerSamplePairedEndFastqDirFmt 5.2 Visualizing artifact files It’s good practice to always visualize your data after manipulating it to see if any reads were lost. You can do this by creating a QIIME2 summary report file (.QZV). First start by making a directory called summaries to store all of your summary files, there will be a lot of them. 5.2.1 Create a summary report Create visualization (.QZV) files out of the .QZA files we just made with a summary command in this summaries.sh script. You can also add this qiime demux summarize command to the end of the previous import.sh job script. #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-name&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 # for bird samples qiime demux summarize \\ --i-data /data/amplicon-analysis/PEreads_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/PEreads-birds.qzv # for sediment samples qiime demux summarize \\ --i-data /data/amplicon-analysis/PEreads_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/PEreads-sediments.qzv Now you can exit the cluster and re-enter using the SFTP to pull your .QZV files off of the cluster and view the resulting artifact file contents on the QIIME2-view Web-App. 5.2.2 Using QIIME2’s web-app To use QIIME2-View, simply drag and drop the file onto the view.qiime2.org dashboard, and take a look at how many reads are in each file. While it can be kind of repetitive to log on and off of the cluster just to view files this way, this SFTP method is used often in the QIIME2 analysis when you use a computing cluster as it allows you to see how many reads are lost during the read joining and quality filtering stages. The results from the quality checking step that is covered in the next section can also be used to compare the results from the QIIME2 summary command that was just used. To read more about using the viewing web-app visit the QIIME2 View Info Page.   Github: johannabosch yohannabosch@gmail.com   "],["data-preprocessing.html", "6 DATA PREPROCESSING 6.1 Quality assessment 6.2 Data Cleaning 6.3 Data transformation 6.4 Data reduction 6.5 Manipulating tables", " 6 DATA PREPROCESSING Before conducting any further analysis such as assigning taxonomy to read files, it is important to conduct a few pre-processing steps on your files to clean up the data. Here are the 4 basic data pre-processing steps to follow: 1 - Data quality assessment 2 - Data cleaning 3 - Data transformation 4 - Data reduction 6.1 Quality assessment Inspecting the quality of the .FASTQ files is very important. You must inspect the quality of your data (phred score, number of unique to duplicate reads per file, number of total reads, GC content, etc.) before running any analysis so you can consider the data quality throughout all aspects of the analysis. If you want to inspect data sets that contain multiple samples, Fit’s nice to be able to review all the samples together - this also makes it easier to understand what methods may or may not be working for sequencing/library prep. Here, we use 2 tools for quality checking multiple samples at once: FastQC [6] to inspect the read quality of each sample file individually (R1 and R2) MultiQC [7] to visualize data quality for all the samples together 6.1.1 FastQC First, inspect the read quality of each sample individually. Here I separated the quality check for seabird fecal samples and sediment samples. Create a directory in both /rawdata/birds and rawdata/sediments called fastqc_out # navigate to the main directory mkdir rawdata/birds/fastqc_out mkdir rawdata/sediments/fastqc_out Run the fastqc command by opening nano and writing a script: NOTE: make sure to use the newest version of the fastqc module (in this case v0.11.9). Use module spider fastqc to view available versions. [copy and paste the following batch script into nano and then edit accordingly] #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-name&gt; #SBATCH --mem= module load StdEnv/2020 fastqc/0.11.9 #run the quality check on bird samples fastqc rawdata/birds/*.fastq.gz rawdata/birds/fastqc_out #run the quality check on sediment samples fastqc rawdata/sediments/*.fastq.gz rawdata/sediments/fastqc_out Save the script as fastqc.sh and run the job script using the sbatch command: sbatch fastqc.sh # view the output of the job view slurm-&#39;jobID&#39; # view the files in the new directories ls rawdata/birds/fastqc_out ls rawdata/sediments/fastqc_out NOTE: When you finish running a job, and have looked at the output slurm file to verify that the command performed well, then you can delete the slurm file using the rm slurm* command. Otherwise your slurm files will build up over time and it will be confusing! 6.1.2 MultiQC Next, I inspected read quality of all the samples together using the following MultiQC commands on both birds and sediments. I specified the output to the file with the -o argument, to the rawdata/birds and rawdata/sediments directories. To install MultiQC load python and load a virtual environment within your job script, like this: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-name&gt; #SBATCH --mem= module load python # load a virtual environment virtualenv ~/my_venv source ~/my_venv/bin/activate # use pip to install MultiQC pip install --no-index multiqc # run the MultiQC commands multiqc rawdata/birds/fastqc_out/ -o rawdata/birds/ multiqc rawdata/sediments/fastqc_out/ -o rawdata/sediments Save the script as ‘multiqc.sh’, and run it using the sbatch command: sbatch multiqc.sh # check to make sure the multiqc files were generated properly ls rawdata/birds/fastqc_out ls rawdata/sediments/fastqc_out A multiqc_data directory and multiqc_report.html will be created when running this command. At this point, the HTML version of the MultiQC report can be exported onto a local computer using SFTP to visualize the quality of the data on a web browser. # exit the cluster and re-enter through the sftp exit sftp &#39;username&#39;@graham.computecanada.ca #move to a file on your local directory where you want to keep the quality data files lcd Desktop/ #get the files from the **amplicon-analysis/rawdata** directory, name them accordingly get amplicon-analysis/rawdata/birds/fastqc_out/multiqc_report.html multiqc_report_birds.html get amplicon-data/rawdata/sediments/fastqc_out/multiqc_report.html multiqc_report_sediments.html Exit SFTP and proceed to visualizing the quality of your data 6.1.3 Visualizing data quality Navigate to the files on your local computer and open the data files via your chosen web browser. 6.1.4 Plot types MultiQC report files contain several different ways of interpreting data quality, including plots for. To view some helpful interpretations of these plots, visit ()[]. 6.1.5 For this study We pulled 2 important files from the cluster onto our local computer, the multiqc_report_sediments.html and multiqc_report_birds.html. The quality of the data used in this analysis is reviewed in Bosch [1], but there are a few things to keep in mind: Overall these samples have a decent quality, as exhibited by Phred scores in the Mean Quality Scores plot, which shows that most of the R2 files are above 20 up to approx. 240 bp. Keep this information in mind when trimming the read files later on based on sequence quality. There is a high percentage of duplicate reads across both sediment and bird samples, which is something to keep in mind for downstream analysis. This can usually be cleaned up by removing contaminants, filtering ASVs and rarefying the data for certain diversity analyses. GC content for these data should be between 40 - 60%, here it hovers around 50 - 55% between samples 6.2 Data Cleaning Once the sequencing data was imported to an artifact file, all the primer sequences (and all the reads without primer sequences) can be removed from reads corresponding to the 16S V4/V5 regions (Forward:GTGYCAGCMGCCGCGGTAA, Reverse: CCGYCAATTYMTTTRAGTTT) using the q2-cutadapt plug-in [8]. 6.2.1 Trimming primers The QIIME2 cut-adapt plug-in [8] allows you to work with the adapter sequences within your files. To begin, make sure you’re logged back in to the cluster and in the main directory (/amplicon-analysis). Copy and paste the job script to nano and call it trimming_birds.sh: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 # run the cutadapt command to trim primers from all bird sample reads qiime cutadapt trim-paired \\ --i-demultiplexed-sequences /data/amplicon-analysis/PEreads_birds.qza \\ --p-front-f GTGYCAGCMGCCGCGGTAA \\ --p-front-r CCGYCAATTYMTTTRAGTTT \\ --p-discard-untrimmed \\ --p-no-indels \\ --o-trimmed-sequences /data/amplicon-analysis/trimmed-reads_birds.qza # generate a summary report to see how many reads were lost qiime demux summarize \\ --i-data /data/amplicon-analysis/trimmed-reads_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/cutadapt_birds.qzv Here is a review of some of the parameters (--p) I used: --p-front-f GTGYCAGCMGCCGCGGTAA: a parameter flag that specifies the forward primer sequence used for trimming, GTGYCAGCMGCCGCGGTAA. These should be specified on the information provided by the type of primer used. --p-front-r CCGYCAATTYMTTTRAGTTT: specifies the reverse primer sequence used for trimming, CCGYCAATTYMTTTRAGTTT. --p-discard-untrimmed: indicates that any sequences that are not trimmed (i.e., no matching primer sequences found) should be discarded. --p-no-indels: indicates that the trimming should not allow for insertions or deletions (indels) during the trimming process, meaning the primers must match exactly without any additional or missing bases. Now run the same commands for the sediment samples and call the nano script trimming_sediments.sh: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 qiime cutadapt trim-paired \\ --i-demultiplexed-sequences /data/amplicon-analysis/PEreads_sediments.qza \\ --p-front-f GTGYCAGCMGCCGCGGTAA \\ --p-front-r CCGYCAATTYMTTTRAGTTT \\ --p-discard-untrimmed \\ --p-no-indels \\ --o-trimmed-sequences /data/amplicon-analysis/trimmed-reads_sediments.qza qiime demux summarize \\ --i-data /data/amplicon-analysis/trimmed-reads_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/cutadapt_sediments.qzv Next, I saved and ran the scripts using sbatch and checked the slurm file when the job was complete sbatch trimming_birds.sh sbatch trimming_sediments.sh #check the 2 slurm files that were produced after a few minutes view slurm-&lt;&quot;JOB-ID&quot;&gt; TIP: To view explanations for each option of the cutadapt command or view other options run the help command for cutadapt (or for other tools in the future): qiime cutadapt --help 6.2.2 Summarizing data At this point, exit the cluster and re-enter using the SFTP to move the .QZV files that were just created from the Graham cluster to our local computer. Drag and drop the file onto https://view.qiime2.org/ to view an interactive quality plot and a table of sequence details. REVIEW WHAT THE QIIME PAGE LOOKS LIKE It’s good to make sure there was not an unordinary amount of reads lost as this stage and at the next stages when joining the reads, constructing ASVs and filtering ASVs. 6.2.3 Joining read pairs Since we are working with paired-end data, it’s important to join the read pairs before assigning taxonomy. We can use a merging command offered by the q2-vsearch plug-in [9], which also provides methods for clustering and dereplicating features and sequences. The merge-pairs command can be run to join the R1 and R2 read pairs for each sample. Here, I just incorporated the qiime demux summarize command into the job script. Open nano and use this joining.sh script to join pairs: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 # commands for joining reads of all bird samples qiime vsearch merge-pairs \\ --i-demultiplexed-seqs /data/amplicon-analysis/trimmed-reads_birds.qza \\ --o-joined-sequences /data/amplicon-analysis/joined-reads_birds.qza #use the demux command to generate a summary report for the joining qiime demux summarize \\ --i-data /data/amplicon-analysis/joined-reads_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/joining-summary_birds.qzv #commands for joining reads of all sediment samples qiime vsearch merge-pairs \\ --i-demultiplexed-seqs /data/amplicon-analysis/trimmed-reads_sediments.qza \\ --o-joined-sequences /data/amplicon-analysis/joined-reads_sediments.qza qiime demux summarize \\ --i-data /data/amplicon-analysis/joined-reads_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/joining-summary_sediments.qzv sbatch joining.sh 6.2.4 Quality filtering Finally, it is always good practise to filter your data based on quality, I wrote a job script to run a quality filter on the joined reads using the qiime quality-filter command [10] and created a summary report of the check using the qiime demux sumamrize command nano qualityfilter.sh Open nano and use the following job script titled quality-filter.sh, edit accordingly, and run using sbatch: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for bird samples qiime quality-filter q-score \\ --i-demux /data/amplicon-analysis/joined-reads_birds.qza \\ --o-filter-stats /data/amplicon-analysis/filt_stats_birds.qza \\ --o-merged-sequences /data/amplicon-analysis/filteredreads_birds.qza qiime demux summarize \\ --i-data /data/amplicon-analysis/filteredreads_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/filteredreads_birds.qzv #for sediment samples qiime quality-filter q-score \\ --i-demux /data/amplicon-analysis/joined-reads_sediments.qza \\ --o-filter-stats /data/amplicon-analysis/filt_stats_sediments.qza \\ --o-merged-sequences /data/amplicon-analysis/filteredreads_sediments.qza qiime demux summarize \\ --i-data /data/amplicon-analysis/filteredreads_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/filteredreads_sediments.qzv Now that we have trimmed the primers, joined the read files and quality filtered the data, we can begin transforming the data by constructing amplicon sequence variants (ASV’s) and assigning taxonomy to our samples using a reference database. 6.3 Data transformation In the context of amplicon sequence analysis, data transformation refers to the manipulation and organization of raw sequencing data into a format that can be used for downstream analysis. This includes steps such as quality filtering, denoising, and assigning unique sequence variants to create an Amplicon Sequence Variant (ASV) table. Transforming the raw sequencing data allows us to explore and interpret microbial community composition and dynamics. 6.3.1 Constructing ASV’s ASV (Amplicon Sequence Variant) analysis is a newer approach used in microbial community studies [11], particularly in the analysis of 16S rRNA gene sequencing data - it is different from the more well-known Operational Taxonomic Unit (OTU) approach. ASV analysis sees each unique sequence variant as its own special biological entity. Even a tiny difference of just one nucleotide between two sequences means they become separate ASVs. It’s different from the traditional way of grouping sequences into OTUs (Operational Taxonomic Units) based on a similarity threshold of around 97%. With ASVs, we get a sharper resolution, picking up even the tiniest genetic variations in the microbial community. By keeping the uniqueness intact, ASVs can give us more precise taxonomic assignments. While both methods, ASV and OTU, are accepted and widely used in microbial ecology, there has been a growing preference for ASVs due to their ability to distinguish closely related taxa and to reduce the impact of sequencing errors and noise in the data. That means we can get more sensitive and biologically meaningful results. To begin, the reads from each joined-pair file are corrected and then amplicon sequence variants can be constructed for each file using the qiime deblur plug-in [12] for 16S data. Here, I used a trimming length (--p-trim-length) equal to 240 based on the quality score of the reads that were obtained in the previous section. Use this job script to run the ASV construction on both sediment and bird samples, and run the qiime feature-table summary command to create a .QZV file that lists the number of ASV’s in each feature table: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for sediment core sub-samples qiime deblur denoise-16S \\ --i-demultiplexed-seqs /data/amplicon-analysis/filteredreads_sediments.qza \\ --p-trim-length 240 \\ --p-sample-stats \\ --o-representative-sequences /data/amplicon-analysis/rep-seqs_sediments.qza \\ --o-table /data/amplicon-analysis/feature-table_sediments.qza \\ --o-stats /data/amplicon-analysis/deblurstats_sediments.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/feature-table_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/feature-table_sediments.qzv #for seabird fecal swab samples qiime deblur denoise-16S \\ --i-demultiplexed-seqs /data/amplicon-analysis/filteredreads_birds.qza \\ --p-trim-length 240 \\ --p-sample-stats \\ --o-representative-sequences /data/amplicon-analysis/rep-seqs_birds.qza \\ --o-table /data/amplicon-analysis/feature-table_birds.qza \\ --o-stats /data/amplicon-analysis/deblurstats_birds.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/feature-table_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/feature-table_birds.qzv At this point, there should be 10 files in total produced by this script (5 for each sample type), including: File Description deblur.log a job file that lists processing details, as well as the total sequences, passing sequences, and failing sequences deblurstats.qza statistical information on the ASV construction (can be view using the following command: qiime metadata tabulate --m-input-file deblurstats.qza --o-visualization deblurstats.qzv ) rep-seqs.qza this file contains all the sequences associated to each ASV (multiple sequences can be associated to one ASV) featuretabledeblur.qza this file contains a table of all the ASV’s associated to each sample deblurtablesummary.qzv summary file created from the qiime feature-table summary command 6.4 Data reduction Data reduction is the last data preprocessing step covered in this tutorial. Here, data is reduced based on the confidence of taxonomic classifications, level of classification and uniqueness of the assignments. 6.4.1 Filtering unique reads First, filter out ASVs based on total frequency of samples, which removes all unique reads in the library that are not wanted since they are likely due to Mi-Seq bleed-through between runs (~0.1%). We used the following filter-features command, as a part of the feature-table plug-in to begin filtering our table. For this command, a cut-off of 10 was used for the minimum frequency (--p-min-frequency) option. I decided to set the minimum number of samples a feature must appear in to just 1 sample (–p-min-samples). This way, I made sure to include all samples in our set, even the ones with unique or rare features. After filtering out rare ASVs, a summary command can be run to view how many reads were lost in each file. #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for sediment samples qiime feature-table filter-features \\ --i-table /data/amplicon-analysis/feature-table_sediments.qza \\ --p-min-frequency 10 \\ --p-min-samples 1 \\ --o-filtered-table /data/amplicon-analysis/filtered_feature-table_sediments.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/filtered_feature-table_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/filtered_feature-table_sediments.qzv #for seabird samples qiime feature-table filter-features \\ --i-table /data/amplicon-analysis/feature-table_birds.qza \\ --p-min-frequency 10 \\ --p-min-samples 1 \\ --o-filtered-table /data/amplicon-analysis/filtered_feature-table_birds.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/filtered_feature-table_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/filtered_feature-table_birds.qzv To see a review on some of the parameters you can use for the filter-features command, use” qiime feature-table filter-features --help. Transfer the summary file to your local computer using the SFTP reviewed in Ch-03 (Importing Data), in order to view the results on view.qiime2.org. 6.5 Manipulating tables Knowing how to manipulate QIIME2 feature tables for downstream analysis (concatenating, merging, grouping, collapsing, etc.) is crucial because it allows you to customize and structure microbiome data effectively and extract meaningful insights. This way you can perform accurate statistical analyses tailored to your research questions! 6.5.1 Merging feature tables First let’s create a feature table that merges feature counts from filtered_feature-table_birds.qza and filtered_feature-table_sediments.qza. In the QIIME2 command-line tool, the qiime feature-table merge command allows you to merge multiple feature tables. One thing you’ll also want to do upon merging your feature tables is merge your representative sequence files using the merge-seqs command. This can be done the exact same way as above. Check to make sure the table was merged properly using the summary command by transferring the file to your local computer to view on view.qiime2.org. #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 qiime feature-table merge \\ --i-tables /data/amplicon-analysis/filtered_feature-table_birds.qza /data/amplicon-analysis/filtered_feature-table_sediments.qza \\ --p-overlap-method sum \\ --o-merged-table /data/amplicon-analysis/merged-table.qza qiime feature-table merge-seqs \\ --i-data /data/amplicon-analysis/rep-seqs_birds.qza /data/amplicon-analysis/rep-seqs_sediments.qza \\ --o-merged-data /data/amplicon-analysis/merged_rep-seqs.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/merged-table.qza \\ --o-visualization /data/amplicon-analysis/summaries/merged-table.qzv Make sure that when you are listing your input tables (--i-tables) that there is a space between the two tables you want to merge (this command will not work if you put two --i-tables options). In the above command, the --p-overlap-method parameter specifies how the tool should handle overlapping feature IDs when merging these tables. The reason you have to specify an overlap method is because QIIME2 needs to know how to handle cases where the same feature (i.e., the same ID) appears in multiple input feature tables. This situation can arise when you are merging tables that have the same sample IDs, and it’s important to determine how to handle these overlaps to ensure the accuracy of downstream analyses. The --p-overlap-method parameter offers several options for handling overlapping IDs: average calculates the average value for each overlapping feature ID across all the input tables. error_on_overlapping_feature raises an error if there are overlapping feature IDs in the input tables, indicating that you do not want to allow such overlaps. error_on_overlapping_sample raises an error if there are overlapping sample IDs in the input tables, indicating that you do not want to allow such overlaps. sum will add up the values for each overlapping feature ID across all the input tables. In this case, there are completely different samples in each feature table, so there is no overlap in the sample IDs between the two tables, but there may be overlap in the feature IDs. Using the --p-overlap-method sum option to merge the tables won’t affect the samples in this case.   Github: johannabosch yohannabosch@gmail.com   "],["diversity-analyses.html", "7 DIVERSITY ANALYSES 7.1 Build a phylogenetic tree 7.2 Generating alpha rarefaction curves 7.3 Alpha and beta diversity", " 7 DIVERSITY ANALYSES 7.1 Build a phylogenetic tree While a phylogenetic tree is not technically used to assess diversity within samples, the files generated in this step are used to assess beta-diversity down below. To generate the trees, I use the QIIME2 plug-in SEPP [13-16]. First, acquire the SEPP reference database for 16S data, and store it in the main working directory (amplicon-analysis). You can find the latest versions on the QIIME2 Docs Data Resources page Instead of downloading the database on your local computer and transferring it to the cluster, just copy the link to the reference database you’d like to use, and then use the wget command to download the proper file: wget https://data.qiime2.org/2023.7/common/sepp-refs-silva-128.qza Then use the QIIME2 fragment-insertion command with the SEPP reference database to generate the tree: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #build a tree for seabird fecal samples qiime fragment-insertion sepp \\ --i-representative-sequences /data/amplicon-analysis/rep-seqs_birds.qza \\ --i-reference-database /data/amplicon-analysis/sepp-refs-silva-128.qza \\ --o-tree /data/amplicon-analysis/asvs-tree_birds.qza \\ --o-placements /data/amplicon-analysis/insertion-placements_birds.qza #build a tree for sediment samples qiime fragment-insertion sepp \\ --i-representative-sequences /data/amplicon-analysis/rep-seqs_sediments.qza \\ --i-reference-database /data/amplicon-analysis/sepp-refs-silva-128.qza \\ --o-tree /data/amplicon-analysis/asvs-tree_sediments.qza \\ --o-placements /data/amplicon-analysis/insertion-placements_sediments.qza #build a tree for the merged feature table qiime fragment-insertion sepp \\ --i-representative-sequences /data/amplicon-analysis/merged_rep-seqs.qza \\ --i-reference-database /data/amplicon-analysis/sepp-refs-silva-128.qza \\ --o-tree /data/amplicon-analysis/asvs-tree.qza \\ --o-placements /data/amplicon-analysis/insertion-placements.qza 7.2 Generating alpha rarefaction curves An alpha rarefaction curve is used for understanding the richness and diversity of microbial communities within a given sample or data set. In the context of microbiome analysis, it provides insights into the number of unique species, known as alpha diversity, at different sampling depths. These curves are generated by repeatedly subsampling the sequencing data at various depths, and measuring diversity metrics (e.g., Shannon or Simpson index) at each step. As the sequencing depth increases, the curve typically plateaus, revealing the point at which additional sequencing data no longer significantly contributes to the detection of new microbial taxa. Alpha rarefaction curves not only help researchers identify optimal sequencing depths but also offer a glimpse into the complex and unique microbial ecosystems present in various environments, making them a fundamental tool in microbiome research. Start by making a directory called /diversity to keep all the files that will be generated in this next section. mkdir diversity 7.2.1 Finding max depth To create an alpha-rarefaction curve, first find out what you want the maximum depth (--p-max-depth in the below command) value to be for your data. This might depend on the quality of the data, or your research question. To determine the appropriate value, you should ideally choose a depth that retains a substantial portion of your data set while still ensuring meaningful results. You can start by looking at your filtered feature table to find the maximum sequencing depth present, or you can set it to a value slightly lower to provide a buffer. I chose maximum depth based on the maximum sequencing depth for both sample types found within the summary files that were generated in the last steps of the Data Preprocessing stage: filtered_feature-table_birds.qzv and filtered_feature-table_sediments.qzv. When opening these files in QIIME2 View, this number can be found on the first tab of the visualization file, called Overview, or on the Interactive Sample Detail page. For example, the maximum depth of the bird fecal samples after filtering was 199 289, and the maximum depth for the sediment samples was 33 503. 7.2.2 Generating the curve The values found above for the maximum sampling depths can be used for –p-max-depth in the following command: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 qiime diversity alpha-rarefaction \\ --i-table /data/amplicon-analysis/filtered_feature-table_sediments.qza \\ --p-max-depth 365593 \\ --p-steps 20 \\ --p-metrics &#39;observed_features&#39; \\ --o-visualization /data/amplicon-analysis/diversity/rarefaction_sediments.qzv qiime diversity alpha-rarefaction \\ --i-table /data/amplicon-analysis/filtered_feature-table_birds.qza \\ --p-max-depth 42252 \\ --p-steps 20 \\ --p-metrics &#39;observed_features&#39; \\ --o-visualization /data/amplicon-analysis/diversity/rarefaction_birds.qzv 7.3 Alpha and beta diversity In microbiome analysis, alpha diversity is like looking at the diversity within a single sample or community, telling you how many different types of microbes are present and how evenly they’re distributed. On the other hand, beta diversity is about comparing multiple samples to see how they differ in terms of their microbial composition, helping you understand which microbes are unique to each sample and which are shared. So, alpha diversity focuses on diversity within, while beta diversity looks at differences between different microbial communities. 7.3.1 Running diversity analyses The qiime diversity core-metrics-phylogenetic command can be used to run a suite of diversity analyses on all of the samples in your set all at once, and it will also rarefy all samples to the same sample sequencing depth prior to calculating metrics. Let’s run the qiime diversity core-metrics-phylogenetic command on (1) the seabird fecal samples, (2) the sediment core samples, and (3) the merged feature table that was created in the last steps of Ch-06 (Data Preprocessing). In this command, --p-sampling-depth X represents the depth at which you would like the analysis to be performed at. To retain as many of the samples as possible, I chose the lowest reasonable sample depth taken from the two following summary visualization files: filtered_feature-table_birds.qzv and filtered_feature-table_sediments.qzv. For this part of the analysis a metadata file and phylogenetic tree are also necessary. To view an example of a Q2 metadata file, go back to Ch-01 (Introduction). #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #core metrics for seabird fecal sample qiime diversity core-metrics-phylogenetic \\ --i-table /data/amplicon-analysis/filtered_feature-table_birds.qza \\ --i-phylogeny /data/amplicon-analysis/asvs-tree_birds.qza \\ --p-sampling-depth 42252 \\ --m-metadata-file /data/amplicon-analysis/metadata/metadata-birds.csv \\ --output-dir /data/amplicon-analysis/diversity/diversity_birds #core metrics for sediment samples qiime diversity /data/amplicon-analysis/core-metrics-phylogenetic \\ --i-table filtered_feature-table_sediments.qza \\ --i-phylogeny /data/amplicon-analysis/asvs-tree_sediments.qza \\ --p-sampling-depth 20283 \\ --m-metadata-file /data/amplicon-analysis/metadata/metadata-sediments.csv \\ --output-dir /data/amplicon-analysis/diversity/diversity_sediments # core metrics for all samples together singularity exec -B /project -B /scratch /home/&lt;&quot;user-ID&quot;&gt;/scratch/qiime2-2022.8.sif \\ qiime diversity core-metrics-phylogenetic \\ --i-table /data/amplicon-analysis/merged-table.qza \\ --i-phylogeny /data/amplicon-analysis/asvs-tree.qza \\ --p-sampling-depth 20283 \\ --m-metadata-file /data/amplicon-analysis/metadata/metadata.csv \\ --output-dir /data/amplicon-analysis/diversity/diversity_merged Below I’ve provided a brief explanation of each file. The output artifact (.QZA) files: rarefied_table.qza: This file contains the rarefied feature table, where the sequencing depth is normalized to ensure an equal number of reads per sample. faith_pd_vector.qza: This file contains the Faith’s Phylogenetic Diversity values, a measure of diversity that considers the phylogenetic relationships between species. observed_features_vector.qza: This file contains the number of observed features (e.g., species or OTUs) in each sample. shannon_vector.qza: This file contains Shannon diversity index values, which measure the diversity of species in each sample, considering both abundance and evenness. evenness_vector.qza: This file contains evenness values, indicating how evenly the microbial species are distributed within each sample. unweighted_unifrac_distance_matrix.qza: This file contains the distance matrix computed using the unweighted UniFrac metric, measuring dissimilarity based on the phylogenetic tree. weighted_unifrac_distance_matrix.qza: Similar to the unweighted UniFrac, this file contains the distance matrix computed using the weighted UniFrac metric. unweighted_unifrac_pcoa_results.qza: This file contains the Principal Coordinates Analysis (PCoA) results based on the unweighted UniFrac distance metric. weighted_unifrac_pcoa_results.qza: Similar to the previous one, this file contains PCoA results based on the weighted UniFrac distance metric. NOTE: Weighted UniFrac in PCA analysis takes into account the abundance of organisms, highlighting the influence of the most abundant species, while unweighted UniFrac focuses on presence and absence, emphasizing less abundant organisms in community comparisons. jaccard_distance_matrix.qza: This file contains the distance matrix based on the Jaccard dissimilarity, which measures community dissimilarity based on species presence or absence. bray_curtis_distance_matrix.qza: This file contains the distance matrix based on the Bray-Curtis dissimilarity, which quantifies compositional differences between samples. jaccard_pcoa_results.qza: This file contains PCoA results based on the Jaccard dissimilarity. bray_curtis_pcoa_results.qza: This file contains PCoA results based on the Bray-Curtis dissimilarity. The output visualization (.QZV) files unweighted_unifrac_emperor.qzv: This is a visualization file that can be viewed using Emperor, a tool for exploring and visualizing PCoA results. weighted_unifrac_emperor.qzv: Similar to the previous one, this is a visualization file for PCoA results using the weighted UniFrac metric. jaccard_emperor.qzv: This is a visualization file for PCoA results based on the Jaccard dissimilarity. bray_curtis_emperor.qzv: This is a visualization file for PCoA results based on the Bray-Curtis dissimilarity.   Github: johannabosch yohannabosch@gmail.com   "],["assigning-taxonomy.html", "8 ASSIGNING TAXONOMY 8.1 Classifying ASV’s 8.2 Verifying with BLASTn 8.3 Grouping feature tables", " 8 ASSIGNING TAXONOMY Assigning taxonomy to ASVs (Amplicon Sequence Variants) involves classifying unique genetic sequences into taxonomic categories (e.g., genus or species) to identify the organisms they originate from. This helps researchers understand the microbial composition in a sample based on high-resolution genetic data. To assign taxonomy to ASVs using a reference database, you can use a sequence classification tool like QIIME2’s feature-classifier. In this case, you basically provide the ASV sequences as input, and the tool matches them to known sequences in the reference database to assign taxonomic labels. 8.1 Classifying ASV’s There are three options for classifying ASVs taxonomically in QIIME: 1) feature-classifier classify-sklearn 2) feature-classifier consensus-vsearch 3) feature-classifier classify-consensus-vsearch Here, I used the classify-sklearn approach, which is known for its speed and efficiency, and can be advantageous when working with large data sets. It’s also a well-documented and widely used method, making it a reliable option with ample community support. However, it’s important to note that the best choice may depend on your specific data set and research goals, so it’s always a good idea to consider the strengths and limitations of each classification method before making a decision. 8.1.1 Using scikit-learn Scikit-learn (sklearn) is a powerful machine learning library, including a naive Bayes classifier, used to categorize marker-gene sequences in QIIME based on a pre-trained model (silva-138-99-nb-classifier.qza), revealing the composition of microbial communities in the data set. To begin, a taxonomic classifier was obtained from the full length SILVA reference sequences on the Qiime2 website: (https://docs.qiime2.org/2022.2/data-resources/) Download the classifier into the main directory with the wget command (this can take a long time and can also be done in a job script) wget https://data.qiime2.org/2023.7/common/silva-138-99-nb-classifier.qza Next, run the full length 16S taxonomic classification on the read files with the following qiime feature-classifier: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for sediment core samples qiime feature-classifier classify-sklearn \\ --i-reads /data/amplicon-analysis/rep-seqs_sediments.qza \\ --i-classifier /data/amplicon-analysis/silva-138-99-nb-classifier.qza \\ --o-classification /data/amplicon-analysis/classification_sediments.qza #for seabird fecal swab samples: qiime feature-classifier classify-sklearn \\ --i-reads /data/amplicon-analysis/rep-seqs_birds.qza \\ --i-classifier /data/amplicon-analysis/silva-138-99-nb-classifier.qza \\ --o-classification /data/amplicon-analysis/classification_birds.qza #for the merged file: qiime feature-classifier classify-sklearn \\ --i-reads /data/amplicon-analysis/merged_rep-seqs.qza \\ --i-classifier /data/amplicon-analysis/silva-138-99-nb-classifier.qza \\ --o-classification /data/amplicon-analysis/classification.qza 8.2 Verifying with BLASTn As a safety measure, use the feature-table plug-in [17] to run a tabulate-seqs [18] command that will generate a table with all the Feature ID’s associated to each representative sequence. Using this plug-in, it’s also possible to use BLASTn to evaluate the subset of taxonomic assignments in the taxa files. By comparing the taxonomic assignments from the preceding output to the leading BLASTn hits, you can verify the classifier’s accuracy. Let’s do this for the rep-seqs file for the sediment and seabird fecal samples, as well as the merged_rep-seqs file: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 qiime feature-table tabulate-seqs \\ --i-data /data/amplicon-analysis/rep-seqs_sediments.qza \\ --o-visualization /data/amplicon-analysis/BLAST_sediments_rep-seqs.qzv qiime feature-table tabulate-seqs \\ --i-data /data/amplicon-analysis/rep-seqs_birds.qza \\ --o-visualization /data/amplicon-analysis/BLAST_bird_rep-seqs.qzv qiime feature-table tabulate-seqs \\ --i-data /data/amplicon-analysis/merged_rep-seqs.qza \\ --o-visualization /data/amplicon-analysis/BLAST_merged_rep-seqs.qzv Open the Blast files in QIIME2 and there will be three tables that load: The sequence length statistics The seven-number summary of sequence lengths The sequence table BLAST a sequence against the NCBI nt database by clicking on the linked sequences within the sequence table, which will will open NCBI’s blastn suite. Click on the View Report button and wait for the suite to load the nt data for this sequence (this can take a while). You can do this with around five sequences at random to determine whether or not the results from the taxonomic classification through QIIME2 align with the NCBI database. 8.2.1 Filtering contaminants After assigning taxonomy, it’s possible to remove any contaminants or noise in the data by filtering out unwanted assignments based on the taxonomic labels given to ASVs. Here, I remove any mitochondrial and chloroplast 16S sequences by excluding any ASV which contains those terms in its taxonomic label, including any ASV that is unclassified at the phylum level. We omitted the parameter &lt;--p-include p__&gt; because we are studying a poorly characterized environment where there is potential to identify novel phyla. #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 #for sediment samples qiime taxa filter-table \\ --i-table /data/amplicon-analysis/filtered_feature-table_sediments.qza \\ --i-taxonomy /data/amplicon-analysis/classification_sediments.qza \\ --p-exclude mitochondria,chloroplast \\ --o-filtered-table /data/amplicon-analysis/final_table-sediments.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/final-table_sediments.qza \\ --o-visualization /data/amplicon-analysis/summaries/final_table-sediments.qzv #for bird fecal samples qiime taxa filter-table \\ --i-table /data/amplicon-analysis/filtered_feature-table_birds.qza \\ --i-taxonomy /data/amplicon-analysis/classification_birds.qza \\ --p-exclude mitochondria,chloroplast \\ --o-filtered-table /data/amplicon-analysis/final_table-birds.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/final-table_birds.qza \\ --o-visualization /data/amplicon-analysis/summaries/final_table-birds.qzv #for the merged table qiime taxa filter-table \\ --i-table /data/amplicon-analysis/merged-table.qza \\ --i-taxonomy /data/amplicon-analysis/classification.qza \\ --p-exclude mitochondria,chloroplast \\ --o-filtered-table /data/amplicon-analysis/final_merged-table.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/final_merged-table.qza \\ --o-visualization /data/amplicon-analysis/summaries/final_merged-table.qzv 8.3 Grouping feature tables Next, let’s take the merged feature table and create a grouped feature table that takes the sum of all of the feature counts from each group. This table will be useful later on in the analysis when we are visualizing data. To group all of the samples, you can use the QIIME2 platform to make a feature table that sums up all the feature counts for every featureID according to each sample group with the qiime feature-table group function. This is done using the accompanying metadata.csv files, which specify which samples are a part of which group under the column titled group. To get a better idea of what is meant by the term ‘grouping’, take some of the data we are using here as an example. In this table below, there are 2 northern gannet (Morus_bassanus) samples and 2 black-legged kittiwake (Rissa_tridactyla) samples. In the metadata file, the column group contains the scientific names for each seabird species, so when you run the grouping command and specifiy this column the feature table will be summed up accordingly. Right now this feature table (assigned to the term bird_features) contains feature counts for every sample individually, but we want it to be grouped, so that it looks like Table 2: Table 1. Structure of bird_features file: FeatureID NOGA-01 NOGA-02 BLKI-01 BLKI-02 4595dec6ed… 3498 10022 1244 589 Table 2. Structure of a grouped bird_features file: FeatureID Morus_bassanus Rissa_tridactyla 4595dec6ed… 13520 1833 To group the final_merged-table.qza, use the following command: #!/bin/bash #SBATCH --time= #SBATCH --account=&lt;group-ID&gt; #SBATCH --mem= export APPTAINER_BIND=/project/6011879/&lt;user-ID&gt;/:/data export APPTAINER_WORKDIR=${SLURM_TMPDIR} module load qiime2/2023.5 qiime feature-table group \\ --i-table /data/amplicon-analysis/final_merged-table.qza \\ --p-axis sample \\ --m-metadata-file /data/amplicon-analysis/metadata/metadata.csv \\ --m-metadata-column group \\ --p-mode sum \\ --o-grouped-table /data/amplicon-analysis/grouped-table.qza qiime feature-table summarize \\ --i-table /data/amplicon-analysis/grouped-table.qza \\ --o-visualization /data/amplicon-analysis/summaries/grouped-table.qzv If you’re working the seabird and sediments data, you now have a table that contains all of the data, summed up by each group listed in the metedata-grouped.csv table. ___   Github: johannabosch yohannabosch@gmail.com   "],["visualizing-taxonomic-data.html", "9 VISUALIZING TAXONOMIC DATA 9.1 Exporting data 9.2 Importing Q2 files to R 9.3 QIIME2R 9.4 Phyloseq", " 9 VISUALIZING TAXONOMIC DATA You can build lots of different charts through QIIME2 via the compute canada clusters, like we did with the diversity plots - but I think the QIIME2 plots are not as versatile as the ones that can be built in R. Here, I walk you through the steps of exporting the appropriate data to R to use several different packages that handle QIIME2 artifacts. The figures made in these next visualization sections can be saved as a vector file and then edited in other software such as Illustrator, Power Point, Libre Office, etc. for publishing. If you’re brand new to using R and RStudio, you should become familiar with the interface first. Here are a few links to get started REducation Beginners Tutorial Quantitative Guide for Biology: Intro to R 9.1 Exporting data To begin making the figures, let’s export the data on to a local computer from the cluster using the safe file transfer protocol (SFTP). You can return to Chapter 03 - Importing Data to review how the SFTP works. I exported the following QZA files onto my local computer and kept them in a file called Desktop/QIIME_files. Make sure to put these files into a dedicated folder for the remainder of the analysis so they are easy to find File type Associated file names Final feature table final_table-sediments.qza, final_table-birds.qza, final_merged-table.qza Representative sequences rep-seqs_sediments.qza, rep-seqs_birds.qza, rep_seqs.qza Classification files classification_sediments.qza, classification_birds.qza, classification.qza Phylogenetic tree asvs-tree_sediments.qza, insertion-placements_sediments.qza, asvs-tree_birds.qza, insertion-placements_birds.qza, asvs-tree.qza, insertion-placements.qza Merged feature table merged-table.qza Grouped feature table grouped-table.qza Diversity folder for birds includes all of the files from the qiime core-metrics-phylogenetic command run on the bird samples (put it into a folder named /diversity-birds) Diversity folder for sediments includes all of the files from the qiime core-metrics-phylogenetic command run on the sediment samples (put it into a folder named /diversity-sediments) Diversity folder for merged files includes all of the files from the qiime core-metrics-phylogenetic command run on the merged feature table (put it into a folder named /diversity-merged) 9.2 Importing Q2 files to R A few different packages are used to load the QIIME2 artifacts into R. If you’re new to R and haven’t installed Rtools, do that first. RTools is a separate software that you have to download and install in your system (not in R). Follow this link to install RTools on your system - https://cran.rstudio.com/bin/windows/Rtools/. Sometimes you can get away without doing this step, it doesn’t hurt to try! Begin by installing and loading the following R packages. To install the qiime2R package, I used the remotes package because some versions of R will not allow you to install qiime2R through the normal install.packages method. #install the ggforce and remotes package install.packages(&quot;remotes&quot;) #now load the remotes package library(remotes) #install qiime2R remotes::install_github(&quot;jbisanz/qiime2R&quot;) Now install and load some other packages that will be used in the downstream analysis install.packages(&quot;ggplot2&quot;) install.packages(&quot;ggforce&quot;) install.packages(&quot;tidyverse&quot;) #load the packages library(ggplot2) library(ggforce) library(tidyverse) 9.3 QIIME2R QIIME2R is an R package that was created to manipulate QIIME2 artifact files for downstream analysis. It allows users to extract data from an artifact file, manipulate the data within the files, and create publication quality figures, including taxonomic bar charts, diversity plots, and more. Start by setting your working directory, here I use the file Desktop/QIIME-files setwd(&quot;C:/Users/Hanna/Desktop/QIIME_files&quot;) #check what files are in your wd list.files() To begin using QIIME2R, make sure the package is loaded first. library(qiime2R) 9.3.1 Load the feature table Next, read the feature tables in .QZA format that we will be using for the downstream analysis. Here I use the &lt;- to assign the term bird_features to the file containing the feature counts for every bird sample (found in the folder QIIME-files in my Desktop folder). bird_features&lt;- read_qza(&quot;final_table-birds.qza&quot;) Once the data is read, you can view it using this command: view(bird_features$data) Now do the same for the sediment samples and the grouped feature table that was made in Ch-06 (Data Preprocessing): sed_features &lt;- read_qza(&quot;final_table-sediments.qza&quot;) view(sed_features$data) grouped_features &lt;- read_qza(&quot;grouped-table.qza&quot;) view(grouped_features$data) To view if the information stored in the files is correct you can also use thenames command: names(sed_features) #shows first 5 samples and first 5 taxa names(bird_features) One handy command to know is the write.csv() command, which can be used to create .csv file of your feature table. write.csv(bird_features$data, &quot;feature-table_birds.csv&quot;) 9.3.2 Load the metadata Now that the feature table is loaded into R, the metadata can also be loaded. Always ensure that your metadata file is properly formatted. If it’s unclear how metadata is used in this analysis, you should download the metadata files titled metadata.csv, birds_metadata.csv, and sediments_metadata.csv. Take a look at the formatting of each file, and the type of data that’s attached to each sample. Metadata in microbiome analysis is akin to the footnotes in a research paper; while not essential for understanding the main text, it provides valuable information about the samples, such as site ID, sampling location, and species Latin names. This additional context enhances the comprehensibility and depth of taxonomic bar charts, diversity box plots, and other figures. The first column, labelled #SampleID is a special column header indicating the sample IDs, the other categories are the metadata. Each row represents a sample, with corresponding values for each metadata category. Remember that the specific metadata categories and their names can vary depending on your study and the information you want to associate with your samples. To learn more about using metadata in QIIME2, visit the Metadata in QIIME2 page. Load the metadata into R: #metadata file for only sediments sediment_metadata&lt;-read_q2metadata(&quot;metadata-sediments.csv&quot;) #metadata file for only bird fecal samples bird_metadata&lt;-read_q2metadata(&quot;metadata-birds.csv&quot;) #metadata file containing all the samples metadata&lt;-read_q2metadata(&quot;metadata.csv&quot;) #metadata file containing all the grouped samples #you do not have to use the QIIME2R&#39;s command, grouped_metadata&lt;-read_q2metadata(&quot;metadata-grouped.csv&quot;) head(metadata) #used to show top lines of a metadata file 9.3.3 Load the taxonomic data The next step to visualizing data using QIIME2R is to load the taxonomy data into R. This is done using the same read_qza command that was used before. taxonomy&lt;-read_qza(&quot;classification.qza&quot;) head(taxonomy$data) #used to see the first lines of the taxonomy data file view(taxonomy$data) #used to open to taxonomy file in a new tab 9.3.4 Parsing taxonomic data An important step that should be considered is parsing the classification.qza file to break up the taxonomic levels. This will separate each level of classification given to a feature. Here, the term taxonomy is being assigned to the part of the classification.qza file that contains the data (taxonomy$data). In other words, the term taxonomy now becomes the name of the variable where the parsed taxonomic data will be stored. parse_taxonomy is simply a function in the QIIME2R package specifically designed to extract taxonomic data from QIIME 2 artifacts. For example, a string of taxonomic classifications that looks like the first table shown here will end up looking like the second table: Table 1. Original taxonomic file: FeatureID Taxonomy 4595dec6ed… k__Bacteria;p__Firmicutes;c__Clostridia;o__Clostridiales;f__Lachnospiraceae;g__Roseburia;s__Roseburia intestinalis Table 2. Parsed taxonomic data file: FeatureID Kingdom Phylum Class Order Family Genus 4595dec6ed… Bacteria Firmicutes Clostridia Clostridiales Lachnospiraceae Roseburia To parse the taxonomy file, use the following command: taxonomy&lt;-parse_taxonomy(taxonomy$data) view(taxonomy) #save a .csv formatted version of the file if you want write.csv(taxonomy, &quot;taxa.csv&quot;) NOTE: when you do this, you will replace your original file assigned to the term taxonomy with the new parsed file that was also assigned to the term taxonomy. If you want to keep both files separate, you will have to change the term assigned to the parsed file to something like parsed-taxonomy. 9.3.5 Create a taxa barplot In this next section, I create a taxonomic bar chart which presents the top 10 most abundant phyla across all of my seabird and sediment samples together. This means that the resulting plot will show the top 10 most abundant phyla for every seabird species or sediment sampling site side by side. To do this we have to use the grouped-table.qza file that contains feature counts for both the sediment sampling sites and seabird species that was made in the last steps of Ch-06 (Data Preprocessing). Make sure the following packages are loaded for this next section library(dplyr) library(tidyr) library(qiime2R) Next, load the colors you’d like to use for your bar plot. This can be done in many different ways. You can always use a pre-made color palette that can be loaded through various R packages. Let’s try that first! Start by trying to load the RColorBrewer package and choosing the Spectral palette. Here I specify how many colors are needed for each sample group with n=10. # method 1: loading a color palette install.packages(&quot;RColorBrewer&quot;) library(RColorBrewer) mycolors = c(brewer.pal(name=&quot;Spectral&quot;, n = 10), brewer.pal(name=&quot;BrBG&quot;, n = 10)) If you have more sampling groups than the color palette has colors, then this method will not work. You can also combine color palettes, but at that point I just prefer to specifiy the colors myself. And in many cases, assigning your own unique HEX codes to each group is really useful. In cases where you’re working with a lot of different samples, you’ll need the colors to be distinguishable from one another when sitting side by side in the barplot. By assigning your own colors to groups you can always go back and switch the order of colors in your plot. #method 2: assigning your own colors colors &lt;- c(&quot;#FF0000&quot;, &quot;#FFA500&quot;, &quot;#FFFF00&quot;, &quot;#00FF00&quot;, &quot;#00FFFF&quot;, &quot;#0000FF&quot;, &quot;#800080&quot;, &quot;#FF00FF&quot;, &quot;#FF4500&quot;, &quot;#FF69B4&quot;, &quot;#004445&quot;) This next line of code specifies the dimensions of your plot, which is sometimes necessary for plots that are really large but may not necessary. par(mar=c(1000,1000,1000,1000)) Make sure the grouped-table.qza file (assigned to the term grouped_features) and the metadata.csv file (assigned to the term metadata) is loaded into R. Next, use the summarize_taxa command to summarize the taxonomy file by phylum. This line of code can also be used to summarize data by genera, or any other taxonomic level present in your parsed taxonomy file, simply change the term after the $ in this code. taxasums &lt;- summarize_taxa(grouped_features$data, taxonomy)$Phylum Open this new taxasums file to see what happened here. This command merged the taxonomy file and the grouped feature table, and replaced the feature ID’s with their associated taxa, meanwhile keeping the feature counts present for each group. Now use this command to plot the bar graph. Here, I specify that the data should be plotted according to the group column in the file metadata.csv, and that 20 phyla should be plotted to the graph. taxa_barplot(taxasums, grouped_metadata, &quot;group&quot;, ntoplot=10) + scale_fill_manual(values = colors) + #add colors we chose to the plot theme(text = element_text(size = 12)) + #add a theme to change font size guides(fill=FALSE) #remove this line if you want a legend Figure 9.1: Raw version of the taxa barplot showing the top 10 most abundant phyla present in each sampling group Save the figure as an .SVG file so that you can edit it in another software by ungrouping the single layer file. I like using LibreOffice Draw on Linux, or you can use PowerPoint, Illustrator, etc. If you don’t know how to do this in your chosen software, Google it! ggsave(&quot;taxa_birds.svg&quot;, height = 50, width = 100, units=&quot;cm&quot;) If you’d like to know the exact feature counts used in this graph, create a .csv file here that contains all of the feature counts from the taxasums file write.csv(taxasums, &quot;top-taxa-phyla.csv&quot;) 9.3.6 Create a taxa heatmap Now let’s make two heatmaps that show the top 50 most abundant genera in (1) the sediment samples and (2) the seabird fecal samples. Taxonomic heatmaps that highlight the most abundant genera in your data set are important in microbiome studies because they provide a quick and visually intuitive way to identify the key microbial taxa driving differences between samples or experimental groups. These heat maps help researchers pinpoint which genera are significantly contributing to variations in microbial composition, aiding in the discovery of potential biomarkers or ecological patterns. Additionally, they simplify the complex data by focusing on the most relevant taxa, making it easier to interpret and communicate findings to a broader audience. Here’s a review on how to set up the data for the taxonomic heatmaps. If you’re having trouble understanding some of these concepts, revisit the previous section on taxonomic bar plots for a more thorough review on the commands. #summarize the taxa based on genera for each sample type taxasums_birds &lt;-summarize_taxa(bird_features$data, taxonomy)$Genus taxasums_sediments &lt;-summarize_taxa(sed_features$data, taxonomy)$Genus #check the new taxasums files view(taxasums_birds) view(taxasums_sediments) #save the files in .csv format for your analysis write.csv(taxasums_birds, &quot;taxa_counts_birds_genera.csv&quot;) write.csv(taxasums_sediments, &quot;taxa_counts_sediments_genera.csv&quot;) #you can specify what metadata columns you want to use like this species &lt;- bird_metadata$species site &lt;- sediment_metadata$site Now that we’re all set up, let’s create a taxonomic heatmap using the taxa_heatmap command for all of the seabird fecal samples. We start by using the summarized taxa file that was created above (taxasums_birds), and then specify the accompanying metadata file (bird_metadata) to be used, with the specific column we want to use for the x-axis of the heatmap (species). Next, we want to plot 50 taxa (ntoplot=50), and normalize the data with a log10(percent), which emphasizes relative differences between taxa while reducing the influence of extreme values, making it easier to visualize patterns and compare abundance across taxa. Before running this command make sure you have enough space in your Plots quadrant on your screen has enough space to accommodate the heatmap. taxa_heatmap(taxasums_birds, bird_metadata, &quot;species&quot;, ntoplot=50, normalize = &quot;log10(percent)&quot;) + scale_fill_gradient2(low=&quot;#9ac094&quot;, mid=&quot;#5ba14f&quot;, high=&quot;#003510&quot;, midpoint=0, na.value=&quot;white&quot;) + scale_y_discrete(labels = function(x) str_replace(x, &quot;.*;&quot;, &quot;&quot;)) + theme_light() + theme(axis.text.x = element_text(size = 12, angle =-75)) Make sure you have the Plot tab for the bottom right quadrant completely expanded to accommodate the size of the plot. After expanding the quadrant and running this code, you’ll notice that the y-axis takes up most of the space on the plot. Here is what the y-axis labels look like right now: If you do not want to include the entire taxonomic lineage in the y-axis of the figure, you can simply add the scale_y_discrete argument to your command. This line of code modifies the y-axis labels of a discrete scale in a plot by removing everything before and including the last semicolon in each label using the str_replace function from the stringr package. taxa_heatmap(taxasums_birds, bird_metadata, &quot;species&quot;, ntoplot=50, normalize = &quot;log10(percent)&quot;) + scale_fill_gradient2(low=&quot;#9ac094&quot;, mid=&quot;#5ba14f&quot;, high=&quot;#003510&quot;, midpoint=0, na.value=&quot;white&quot;) + scale_y_discrete(labels = function(x) str_replace(x, &quot;.*;&quot;, &quot;&quot;)) + theme_light() + theme(axis.text.x = element_text(size = 12, angle =-75)) Figure 9.2: Raw version of the taxa heatmap showing the top 50 most abundant genera present in each seabird fecal sampling group Let’s do the same with the sediment samples: taxa_heatmap(taxasums_sediments, sediment_metadata, &quot;site&quot;, ntoplot=50, normalize = &quot;log10(percent)&quot;) + scale_fill_gradient2(low=&quot;#c99884&quot;, mid=&quot;#644133&quot;, high=&quot;#351b10&quot;, midpoint=0) + scale_y_discrete(labels = function(x) str_replace(x, &quot;.*;&quot;, &quot;&quot;)) + theme_light() + theme(axis.text.x = element_text(size=12, angle=-75)) Figure 9.3: Raw version of the taxa heatmap showing the top 50 most abundant genera present in each sediment coring site group 9.3.7 Plot beta-diversity PCA (Principal Component Analysis) plots are useful in microbiome analysis because they help visualize complex data by reducing it to a few key dimensions. In the context of comparing microbiomes between sites or species, PCA plots allow us to see patterns and similarities in microbial community composition, making it easier to identify which factors or variables are driving the differences in a data set. While QIIME2 produces some nice interactive PCA plots, it’s difficult to use these for publishing. With the QIIME2R package, you can create beautiful PCA plots and edit them to your liking. Start by assigning the term species and site to the appropriate column from your metadata-birds.csv and metadata-sediments.csv file: species &lt;- bird_metadata$species site &lt;- sediment_metadata$site After running the qiime diversity core-metrics-phylogenetic command in Ch07 (Diversity Analyses), two files called unweighted_unifrac_pcoa_results.qza and weighted_unifrac_pcoa_results.qza were produced for both the sediment and seabird fecal samples (4 files total). These are the files we will use to create the weighted and unweighted PCA plots. NOTE: Weighted UniFrac in PCA analysis takes into account the abundance of organisms, highlighting the influence of the most abundant species, while unweighted UniFrac focuses on presence and absence, emphasizing less abundant organisms in community comparisons. Check that these files are in the appropriate /diversity folders within your working directory, go back to the Exporting data section of this chapter for a quick refresher. Start by loading these files into R, where wunifrac refers to the weighted PCA files, and unwunifrac refers to the unweighted PCA files: wunifrac_birds&lt;-read_qza(&quot;diversity-birds/weighted_unifrac_pcoa_results.qza&quot;) unwunifrac_birds&lt;-read_qza(&quot;diversity-birds/unweighted_unifrac_pcoa_results.qza&quot;) wunifrac_seds&lt;-read_qza(&quot;diversity-sediments/weighted_unifrac_pcoa_results.qza&quot;) unwunifrac_seds&lt;-read_qza(&quot;diversity-sediments/unweighted_unifrac_pcoa_results.qza&quot;) If you want to add a little extra addition to your PCA plots, you can integrate the shannon diversity metrics into the plot. The addition of the Shannon data (shannon) in this code allows you to include the Shannon Diversity Index as a size aesthetic in the PCA plot. By doing so, you can visualize how the diversity of microbial communities (as measured by Shannon Diversity) relates to the distribution of samples in the PCA space. This can help you explore whether samples with higher or lower Shannon diversity values cluster differently in the PCA plot, providing insights into the relationship between diversity and community composition. # Shannon entropy data for sediment core samples shannon_seds&lt;-read_qza(&quot;diversity-sediments/shannon_vector.qza&quot;)$data %&gt;% rownames_to_column(&quot;SampleID&quot;) shannon_values_seds &lt;- shannon_seds$shannon_entropy # Shannon entropy data for seabird fecal samples shannon_birds&lt;-read_qza(&quot;diversity-birds/shannon_vector.qza&quot;)$data %&gt;% rownames_to_column(&quot;SampleID&quot;) shannon_values_birds &lt;- shannon_birds$shannon_entropy The next commands are used to plot the weighted (PCA_w) and unweighted (PCA_unw)PCA’s of the sediment samples. This command can be broken down into several different parts that are explained below: #plotting weighted PCA for sediments PCA_w_seds &lt;- wunifrac_seds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(sediment_metadata) %&gt;% left_join(shannon_seds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = site, size = shannon_values_seds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 1, geom = &quot;polygon&quot;, aes(fill = site), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12)) + xlab(paste(round(100 * wunifrac_seds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * wunifrac_seds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) PCA_w_seds #plotting unweighted PCA for sediments PCA_unw_seds &lt;- unwunifrac_seds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(sediment_metadata) %&gt;% left_join(shannon_seds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = site, size = shannon_values_seds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 12, geom = &quot;polygon&quot;, aes(fill = site), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12)) + xlab(paste(round(100 * unwunifrac_seds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * unwunifrac_seds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) # Set the legend title for size PCA_unw_seds Here is an explanation of some of the arguments in the PCA plotting command: PCA_w_seds &lt;- wunifrac_seds$data$Vectors %&gt;% : This part of the command selects the Vectors data from the wunifrac_seds object and begins a chain of operations. select(SampleID, PC1, PC2) %&gt;% : selects three columns from the data: “SampleID,” “PC1,” and “PC2.” *left_join(sediment_metadata) %&gt;%: performs a left join with the data from sediment_metadata, combining the selected columns from the previous step with the SampleID column. left_join(shannon_seds) %&gt;% : similar to the previous step, performs a left join with the “shannon_seds” data, combining the result of the previous join with this data. ggplot(aes(x = PC1, y = PC2, color = site, size = shannon_values_seds)) : This part starts creating a ggplot, where the color aesthetic is determined by site, and the size aesthetic is determined by shannon_values_seds. stat_ellipse(type = \"norm\", level = 0.95, linetype = 1, geom = \"polygon\", aes(fill = site), alpha = 0.05): This adds ellipses to the plot that represent normal distribution-based confidence intervals (at 95% level) around the points. I chose linetype 1 for weighted PCAs and linetype 12 for unweighted PCAs. xlab(paste(round(100 * wunifrac_seds$data$ProportionExplained[1], 2), \"% - PC1\")) : Sets the x-axis label (and y-axis label in the next line) using the info from the “wunifrac_seds” object that denotes the proportion explained by the first principal component (PC1, or PC2 for the y-axis), converting it to a percentage. Now plot the same for the seabird fecal samples: #plotting weighted PCA for seabird fecal samples PCA_w_birds &lt;- wunifrac_birds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(bird_metadata) %&gt;% left_join(shannon_birds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = species, size = shannon_values_birds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 1, geom = &quot;polygon&quot;, aes(fill = species), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12)) + xlab(paste(round(100 * wunifrac_birds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * wunifrac_birds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) # Set the legend title for size PCA_w_birds #plotting weighted PCA for seabird fecal samples PCA_unw_birds &lt;- unwunifrac_birds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(bird_metadata) %&gt;% left_join(shannon_birds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = species, size = shannon_values_birds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 12, geom = &quot;polygon&quot;, aes(fill = species), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12)) + xlab(paste(round(100 * unwunifrac_birds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * unwunifrac_birds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) # Set the legend title for size PCA_unw_birds Now that we have all 4 plots done for both unweighted and weighted metrics, lets put them all on the same figure using the patchwork package provided by R. install.packages(&quot;patchwork&quot;) library(patchwork) wrap_plots(PCA_w_seds, PCA_unw_seds, PCA_w_birds, PCA_unw_birds) Make sure you expand your bottom left quadrant in RStudio as much as possible to accommodate the plot size, as you can see in this screenshot. Your wrapped PCA figure will look something like this: You can see that we don’t really need two legends for both unweighted and weighted versions of the same plot. Let’s quickly fix the code to remove a legend from the two weighted plots to the left, so that all we have is legends to the right hand side of this figure by adding legend.position=\"none\" to the theme() argument we already have going. # removing the legend for the sediment sample&#39;s weighted PCA PCA_w_seds &lt;- wunifrac_seds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(sediment_metadata) %&gt;% left_join(shannon_seds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = site, size = shannon_values_seds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 1, geom = &quot;polygon&quot;, aes(fill = site), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12), legend.position = &quot;none&quot;) + xlab(paste(round(100 * wunifrac_seds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * wunifrac_seds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) # removing the legend for the seabird fecal sample&#39;s weighted PCA PCA_w_birds &lt;- wunifrac_birds$data$Vectors %&gt;% select(SampleID, PC1, PC2) %&gt;% left_join(bird_metadata) %&gt;% left_join(shannon_birds) %&gt;% ggplot(aes(x = PC1, y = PC2, color = species, size = shannon_values_birds)) + geom_point(alpha = 0.5) + stat_ellipse(type = &quot;norm&quot;, level = 0.95, linetype = 1, geom = &quot;polygon&quot;, aes(fill = species), alpha = 0.05) + theme_q2r() + theme(text = element_text(size = 12), panel.grid.major = element_line(), panel.grid.minor = element_line(), axis.text = element_text(size = 12), legend.position = &quot;none&quot;) + xlab(paste(round(100 * wunifrac_birds$data$ProportionExplained[1], 2), &quot;% - PC1&quot;)) + ylab(paste(round(100 * wunifrac_birds$data$ProportionExplained[2], 2), &quot;% - PC2&quot;)) + labs(size = &quot;Shannon Diversity&quot;) # Set the legend title for size Now wrap the plots one more time to see the final result! wrap_plots(PCA_w_seds, PCA_unw_seds, PCA_w_birds, PCA_unw_birds) Figure 9.4: Raw version of the weighted PCA (two left plots) and unweighted PCA (two right plots) for both sediment samples (top) and seabird fecal samples (bottom) 9.4 Phyloseq Phyloseq is a handy tool in microbiome analysis that helps researchers work with and visualize microbial community data [19]. It can be used alongside QIIME2 artifact files by converting QIIME2 outputs into a Phyloseq object. Like QIIME2R, it allows for advanced analysis, visualization, and exploration of microbiome data within R’s programming environment. install and load the needed packages install.packages(&quot;phyloseq&quot;) library(phyloseq) library(ggplot2) library(tidyverse) library(qiime2R) 9.4.1 Build a phyloseq object Next, create a phyloseq object for your data. These objects can contain several different .QZA files. In this case, they include the feature table, taxonomy file and metadata file, but they can also include the trees, insert-placements and other files which were created throughout the preprocessing and diversity analysis stages. In these next commands, I build 2 phyloseq objects for (1) the seabird fecal samples, (2) the sediment samples. Make sure you are in the correct working directory, in this case /QIIME_files. #building an object for seabird fecal samples birds_physeq&lt;-qza_to_phyloseq( features=&quot;final_table-birds.qza&quot;, tree=&quot;asvs-tree_birds.qza&quot;,&quot;classification_birds.qza&quot;, metadata = &quot;metadata-birds.csv&quot;) birds_physeq #building an object for sediment core samples sediments_physeq&lt;-qza_to_phyloseq( features=&quot;final_table-sediments.qza&quot;, tree=&quot;asvs-tree_sediments.qza&quot;,&quot;classification_sediments.qza&quot;, metadata = &quot;metadata-sediments.csv&quot;) sediments_physeq 9.4.2 Taxonomic characteristics If you want to find out how many taxa have been classified at different taxonomic levels within these objects, use the following commands: #for seabird fecal samples tax_table(birds_physeq) %&gt;% as(&quot;matrix&quot;) %&gt;% as_tibble(rownames = &quot;OTU&quot;) %&gt;% gather(&quot;Rank&quot;, &quot;Name&quot;, rank_names(birds_physeq)) %&gt;% na.omit() %&gt;% # remove rows with NA value group_by(Rank) %&gt;% summarize(ntaxa = length(unique(Name))) %&gt;% # computes number of unique taxa mutate(Rank = factor(Rank, rank_names(birds_physeq))) %&gt;% arrange(Rank) #for sediment samples tax_table(sediments_physeq) %&gt;% as(&quot;matrix&quot;) %&gt;% as_tibble(rownames = &quot;OTU&quot;) %&gt;% gather(&quot;Rank&quot;, &quot;Name&quot;, rank_names(sediments_physeq)) %&gt;% na.omit() %&gt;% group_by(Rank) %&gt;% summarize(ntaxa = length(unique(Name))) %&gt;% mutate(Rank = factor(Rank, rank_names(sediments_physeq))) %&gt;% arrange(Rank) The above command takes the taxonomic table in the birds_physeq object and performs a series of data manipulation operations, using the dplyr and tidyr packages in R. It first converts the taxonomic table to a matrix, then converts it to a tibble with row names set as OTU (used as a generic term here even though I am using ASVs). It gathers the data to create two columns, Rank and Name, from the taxonomic ranks present in birds_physeq. Thena.omit() line removes rows with missing values, and then group_by() line groups the data by Rank, then calculates the number of unique taxa for each rank using the summarize() command. Finally, the mutate() command converts Rank into a factor variable with predefined rank names, and finally arranges the data by Rank in ascending order using the arrange() command. The output for the command will look something like this (these numbers are from running the command on the sediment sample phyloseq object): # A tibble: 7 × 2 Rank ntaxa &lt;fct&gt; &lt;int&gt; 1 Kingdom 2 2 Phylum 57 3 Class 125 4 Order 244 5 Family 309 6 Genus 393 7 Species 160 9.4.3 Plot alpha-diversity What’s nice about Phyloseq is that you can calculate and plot diversity metrics all in one command, so you actually don’t even have to run any qiime diversity commands if you don’t want to! I like the plots that this Phyloseq package produces more anyways, and they are easier to save as an .SVG image to edit later on for publishing. Here I plot three diversity metrics together for the sediment samples according to site, using the following plot_richness command: sediments_richness &lt;- plot_richness(sediments_physeq, x = &quot;site&quot;, color=&quot;depth&quot;, nrow=1, measures = c(&quot;Simpson&quot;, &quot;Shannon&quot;, &quot;Observed&quot;)) + geom_point(size=2, alpha=0.5) + geom_boxplot(alpha=0.3, size=0.3, colour=&quot;black&quot;) + theme_bw()+ theme(text = element_text(size = 10), panel.grid.major.x=element_line(FALSE), axis.text = element_text(size=10), axis.text.x=(element_text(angle=90, vjust=0.5))) The above command includes metrics for Simpson, Shannon, and observed features as subplots. I assign the plot to the term sediments_richness so that I can merge the diversity plots for both sediment and seabird samples together afterwards. Here is a quick review of the arguments I used here: x=\"site\" will split the plot up into the 3 coring sites listed in the metadata file of the phyloseq object color=depth will color the points on the box plot so that diversity can be visualized across different depths of the core. The nrow argument let’s you adjust the orientation of the plots, with 1 row the plots will end up in a horizontal line, with 3 rows the plots will end up in a vertical line. The geom_points() and geom_boxplot() arguments let you manipulate the size and transparency (alpha) of the box plot’s features. When adding a theme to boxplot, there are several options that you can choose from (to see a list of the themes visit this link). I chose the black and white theme (theme_bw()) for my plot, and then on the last line I use the theme() argument to specify the overall text size of the boxplot and legend, as well as the axis text size. The panel.grid.major.x=element_line(FALSE) argument to removes the vertical grid lines as they aren’t necessary here. The panel.grid.major.y=element_line() argument can also be used to specify the color(), linetype() and size() of your grid lines. The axis.text.x=(element_text(angle=90, vjust=0.5) argument changes the orientation of the x-axis text so that it all fits better in the plot using the angle option, and the vjust option to adjust the vertical position of the labels so they are not overlapping the axis. Next, do the same for the seabird fecal samples. This time plotting by species, without any colors for the points. birds_richness &lt;- plot_richness(birds_physeq, x = &quot;species&quot;, nrow=3, measures = c(&quot;Simpson&quot;, &quot;Shannon&quot;, &quot;Observed&quot;)) + geom_point(size=2, alpha=0.5) + geom_boxplot(alpha=0.3, size=0.3, colour=&quot;black&quot;) + theme_bw()+ theme(text = element_text(size = 10), panel.grid.major.x=element_line(FALSE), axis.text = element_text(size=10), axis.text.x=(element_text(angle=90, vjust=0.5))) Now we can make a larger plot for publishing that puts all of these plots together in one figure. You can always do this with plots using the patchwork package in R. install.packages(&quot;patchwork&quot;) library(patchwork) wrap_plots(birds_richness, sediments_richness, nrow=1) Your final box plot should look like this: Figure 9.5: Raw version of the taxa boxplot showing alpha-diversity metrics for seabird fecal samples (left) and sediment samples (right) If you want to use other diversity metrics for a plot like this, Phyloseq offers all of the following metrics: Observed: The number of distinct OTUs (Operational Taxonomic Units) or taxa observed in a sample, reflecting species richness. Chao1: An estimate of total species richness, taking into account both observed species and the number of rare or singleton species. It provides a more robust estimate of richness. ACE (Abundance-based Coverage Estimator): Similar to Chao1, it estimates total species richness while considering the abundance distribution of rare species. It is often used in cases where there are many rare species. Shannon: Measures both richness and evenness of species in a sample. Higher values indicate higher diversity, considering both species number and their relative abundances. Simpson: Focuses on the dominance of a few species in a sample. Lower values indicate higher diversity, with less dominance by a single species. InvSimpson (Inverse Simpson): Similar to Simpson but transformed to emphasize the dominance of species. Higher values indicate higher diversity. Fisher: A rarefaction-based diversity index that estimates species richness while considering the number of individuals sampled. It is sensitive to rare species and is used to compare diversity between samples with different sampling efforts.   Github: johannabosch yohannabosch@gmail.com   "],["resources.html", "10 Resources: 10.1 Data in this analysis 10.2 Other resources", " 10 Resources: Here are some resources that are helpful for running the analysis 10.1 Data in this analysis Download to data from this analysis Github repo (See Bosch et al. [2]) To download the metadata files: https://github.com/johannabosch/QIIME2_for_Graham/tree/main/metadata To download the 2 rawdata files: https://drive.google.com/drive/folders/1mwfqLjyuLIpHO0KuhRAuxgYXjnbp1iQH?usp=sharing 10.2 Other resources This analysis uses various software and tools all made available through the Graham cluster. These are some of the coding languages, tools and software we used in this analysis: R/RStudio: R and RStudio are utilized for generating plots using the QIIME2R and Phyloseq packages. R is a statistical programming language widely employed in data analysis and visualization, while RStudio provides an integrated development environment (IDE) for working with R. FastQC: FASTQC is a quality control tool used to assess the quality of sequencing data. MultiQC: MultiQC is a tool that aggregates results from multiple bioinformatics analyses into a single, comprehensive report to evaluate and summarize quality control metrics. QIIME2: QIIME2 is a powerful microbiome analysis platform used extensively in this analysis. It provides a comprehensive suite of tools for processing, analyzing, and visualizing microbiome data. Near the end of this analysis, we export the final files from the cluster to a local computer (your computer) and use two different R packages locally to run a few other analyses and plot a taxonomic heatmap and barplot. While QIIME2 does offer heatmap/barplot capabilities, the aesthetic features are more limited than QIIME2R and Phyloseq. To use QIIME2R and Phyloseq on your local computer, make sure you have R and RStudio installed. QIIME2R: QIIME2R is an R package that enables integration between QIIME2 and R. It allows importing QIIME2 artifacts and visualizing data within the R environment, and offers further data exploration beyond what QIIME2 currently offers. Phyloseq: Phyloseq is an R package that works seamlessly with QIIME2 data and provides a flexible framework for modifying and visualizing data. Some Digital Alliance pages that are worth checking out: Storing data on the cluster https://docs.alliancecan.ca/wiki/Storage_and_file_management#Best_practices Using Apptainer on a cluster: https://docs.alliancecan.ca/wiki/Apptainer Using QIIME2 on a cluster: https://docs.alliancecan.ca/wiki/QIIME   Github: johannabosch yohannabosch@gmail.com   "],["acknowledgments.html", "11 Acknowledgments", " 11 Acknowledgments This analysis was supported in part by the Digital Research Alliance of Canada. I would to acknowledge the excellent support team at the Alliance for their help, as well as the creators of QIIME2, the QIIME2 forum and the various tools available through the QIIME2 plug-ins. I would also like to acknowlege the Langille Lab of Dalhousie University for their documentation on 16S Microbiome analysis (Microbiome Helper). This work is a part of Bosch et al. 1 and supports the research of the Paleoecological Laboratory in Memorial University of Newfoundland.   Github: johannabosch yohannabosch@gmail.com   "],["technical-support.html", "12 Technical Support", " 12 Technical Support Have questions about running your analysis on an Alliance cluster? E-mail the Alliance support team if you have trouble with your account or even if you have questions about your job scripts or commands while running an analysis. They have a helpful team of individuals who will reply to your e-mails promptly and point you in the right direction. E-mail Purpose accounts@tech.alliancecan.ca For questions about accounts renewals@tech.alliancecan.ca For questions about account renewals globus@tech.alliancecan.ca For questions about Globus file transfer services cloud@tech.alliancecan.ca For questions about using Cloud resources support@tech.alliancecan.ca For any other questions, including questions related to your bioinformatics analysis NOTE: If you are going to contact support, make sure to read this technical support page first to know what to include in your message: (https://docs.alliancecan.ca/wiki/Technical_support)   Github: johannabosch yohannabosch@gmail.com   "],["references.html", "13 References References", " 13 References References [1] Bosch J. 2023 From Seabirds to Sediments: The ecological footprint of seabirds at a prominent North Atlantic breeding colony tracked using a multi-proxy paleolimnological approach [unpubl.]. Newfoundland, CA: Memorial University of Newfoundland [2] Parada AE, Needham DM, Fuhrman JA. 2016 Every base matters: assessing small subunit rRNA primers for marine microbiomes with mock communities, time series and global field samples: Primers for marine microbiome studies. Environmental Microbiology. 18(5):1403–14. doi.org/10.1111/1462-2920.13023 [3} Walters W, Hyde ER, Berg-Lyons D, Ackermann G, Humphrey G, Parada A, et al. 2016 Improved Bacterial 16S rRNA Gene (V4 and V4-5) and Fungal Internal Transcribed Spacer Marker Gene Primers for Microbial Community Surveys. Bik H, editor. mSystems. 1(1):e00009-15. 10.1128/msystems.00009-15 [4] Bolyen E, Rideout JR, Dillon MR, Bokulich NA, Abnet CC, Al-Ghalith GA, et al. 2019 Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2. Nature Biotechnology 37: 852–857. 10.1038/s41587-019-0209-9 [5] Singularity Developers (2021) Singularity. 10.5281/zenodo.1310023 https://doi.org/10.5281/zenodo.1310023 [6] Andrews, S. 2010 FastQC: A Quality Control Tool for High Throughput Sequence Data. URL: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ [7] Ewels P, Magnusson M, Lundin S, Käller M. 2016 MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics. 1;32(19):3047-8. [8] Martin M. 2011 Cutadapt removes adapter sequences from high-throughput sequencing reads. EMBnet. journal, 17(1):pp–10. 10.14806/ej.17.1.200. [9] Rognes T, Flouri T, Nichols B, Quince C, Mahé F. 2016 Vsearch: a versatile open source tool for metagenomics. PeerJ, 4:e2584. 10.7717/peerj.2584. [10] Bokulich NA, Subramanian S, Faith JJ, Gevers D, Gordon JI, Knight R, Mills DA, and Caporaso GJ. 2013 Quality-filtering vastly improves diversity estimates from illumina amplicon sequencing. Nature methods, 10(1):57. 10.1038/nmeth.2276. [11] Edgar, Robert C. 2016 “UNOISE2: improved error-correction for Illumina 16S and ITS amplicon sequencing.” BioRxiv. 081257. [12] Amir A, McDonald D, Navas-Molina JA, Kopylova E, Morton JT, Xu ZZ, et al. 2017 Deblur rapidly resolves single-nucleotide community sequence patterns. MSystems, 2(2):e00191–16 [13] Eddy SR. 2011 Accelerated profile HMM searches. PLOS Computational Biology, 7(10):1–16. 10.1371/journal.pcbi.1002195. [14] Janssen S, McDonald D, Gonzalez A, Navas-Molina JA, Jiang L, Xu ZZ, et al. 2018 Phylogenetic placement of exact amplicon sequences improves associations with clinical information. mSystems, doi:10.1128/mSystems. 00021-18. [15] Matsen FA, Kodner RB, Armbrust EV. 2010 Pplacer: linear time maximum-likelihood and Bayesian phylogenetic placement of sequences onto a fixed reference tree. BMC Bioinformatics, 11(1):538. [16] Matsen FA, Hoffman NG, Gallagher A, Stamatakis A. 2012 A format for phylogenetic placements. PLOS ONE, 7(2):1–4. URL: https://doi.org/10.1371/journal.pone.0031009, 10.1371/journal.pone.0031009. [17] NCBI Resource Coordinators. 2017 Database resources of the national center for biotechnology information. Nucleic Acids Research, 45(D1):D12–D17. 10.1093/nar/gkw1071. [18] Johnson M, Zaretskaya I, Raytselis Y, Merezhuk Y, McGinnis S, and Madden TL. 2008 Ncbi blast: a better web interface. Nucleic Acids Research, 36(suppl_2):W5–W9. 10.1093/nar/gkn201. [19] McMurdie PJ, Holmes S. 2013 Phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217. doi.org/10.1371/journal.pone.0061217   Github: johannabosch yohannabosch@gmail.com   "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
